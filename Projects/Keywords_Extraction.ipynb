{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "EpAdOkQFHGFn"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/papers.csv', on_bad_lines='skip')"
      ],
      "metadata": {
        "id": "xheRAmE7HRZT"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "56F6TjjVJJsH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "outputId": "e29cdf14-b39d-470f-cc1f-9884bde66300"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     id  year                                              title event_type  \\\n",
              "0     1  1987  Self-Organization of Associative Database and ...        NaN   \n",
              "1    10  1987  A Mean Field Theory of Layer IV of Visual Cort...        NaN   \n",
              "2   100  1988  Storing Covariance by the Associative Long-Ter...        NaN   \n",
              "3  1000  1994  Bayesian Query Construction for Neural Network...        NaN   \n",
              "4  1001  1994  Neural Network Ensembles, Cross Validation, an...        NaN   \n",
              "\n",
              "                                            pdf_name          abstract  \\\n",
              "0  1-self-organization-of-associative-database-an...  Abstract Missing   \n",
              "1  10-a-mean-field-theory-of-layer-iv-of-visual-c...  Abstract Missing   \n",
              "2  100-storing-covariance-by-the-associative-long...  Abstract Missing   \n",
              "3  1000-bayesian-query-construction-for-neural-ne...  Abstract Missing   \n",
              "4  1001-neural-network-ensembles-cross-validation...  Abstract Missing   \n",
              "\n",
              "                                          paper_text  \n",
              "0  767\\n\\nSELF-ORGANIZATION OF ASSOCIATIVE DATABA...  \n",
              "1  683\\n\\nA MEAN FIELD THEORY OF LAYER IV OF VISU...  \n",
              "2  394\\n\\nSTORING COVARIANCE BY THE ASSOCIATIVE\\n...  \n",
              "3  Bayesian Query Construction for Neural\\nNetwor...  \n",
              "4  Neural Network Ensembles, Cross\\nValidation, a...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-6512a315-f151-4787-bf2f-b8f616fbac64\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>year</th>\n",
              "      <th>title</th>\n",
              "      <th>event_type</th>\n",
              "      <th>pdf_name</th>\n",
              "      <th>abstract</th>\n",
              "      <th>paper_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1987</td>\n",
              "      <td>Self-Organization of Associative Database and ...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1-self-organization-of-associative-database-an...</td>\n",
              "      <td>Abstract Missing</td>\n",
              "      <td>767\\n\\nSELF-ORGANIZATION OF ASSOCIATIVE DATABA...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>10</td>\n",
              "      <td>1987</td>\n",
              "      <td>A Mean Field Theory of Layer IV of Visual Cort...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>10-a-mean-field-theory-of-layer-iv-of-visual-c...</td>\n",
              "      <td>Abstract Missing</td>\n",
              "      <td>683\\n\\nA MEAN FIELD THEORY OF LAYER IV OF VISU...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>100</td>\n",
              "      <td>1988</td>\n",
              "      <td>Storing Covariance by the Associative Long-Ter...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>100-storing-covariance-by-the-associative-long...</td>\n",
              "      <td>Abstract Missing</td>\n",
              "      <td>394\\n\\nSTORING COVARIANCE BY THE ASSOCIATIVE\\n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1000</td>\n",
              "      <td>1994</td>\n",
              "      <td>Bayesian Query Construction for Neural Network...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1000-bayesian-query-construction-for-neural-ne...</td>\n",
              "      <td>Abstract Missing</td>\n",
              "      <td>Bayesian Query Construction for Neural\\nNetwor...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1001</td>\n",
              "      <td>1994</td>\n",
              "      <td>Neural Network Ensembles, Cross Validation, an...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1001-neural-network-ensembles-cross-validation...</td>\n",
              "      <td>Abstract Missing</td>\n",
              "      <td>Neural Network Ensembles, Cross\\nValidation, a...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6512a315-f151-4787-bf2f-b8f616fbac64')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-6512a315-f151-4787-bf2f-b8f616fbac64 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-6512a315-f151-4787-bf2f-b8f616fbac64');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-f1d68a00-715b-40c2-8bec-1e1b43bdcb2e\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-f1d68a00-715b-40c2-8bec-1e1b43bdcb2e')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-f1d68a00-715b-40c2-8bec-1e1b43bdcb2e button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 7241,\n  \"fields\": [\n    {\n      \"column\": \"id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2098,\n        \"min\": 1,\n        \"max\": 7284,\n        \"num_unique_values\": 7241,\n        \"samples\": [\n          1466,\n          3336,\n          6755\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"year\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 8,\n        \"min\": 1987,\n        \"max\": 2017,\n        \"num_unique_values\": 31,\n        \"samples\": [\n          1992,\n          1990,\n          2012\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"title\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 7241,\n        \"samples\": [\n          \"Independent Component Analysis for Identification of Artifacts in Magnetoencephalographic Recordings\",\n          \"Near-Maximum Entropy Models for Binary Neural Representations of Natural Images\",\n          \"Nearest-Neighbor Sample Compression: Efficiency, Consistency, Infinite Dimensions\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"event_type\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"Oral\",\n          \"Spotlight\",\n          \"Poster\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"pdf_name\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 7241,\n        \"samples\": [\n          \"1466-independent-component-analysis-for-identification-of-artifacts-in-magnetoencephalographic-recordings.pdf\",\n          \"3336-near-maximum-entropy-models-for-binary-neural-representations-of-natural-images.pdf\",\n          \"6755-nearest-neighbor-sample-compression-efficiency-consistency-infinite-dimensions.pdf\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"abstract\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3923,\n        \"samples\": [\n          \"Recommendation for e-commerce with a mix of durable and nondurable goods has characteristics that distinguish it from the well-studied media recommendation problem. The demand for items is a combined effect of form utility and time utility, i.e., a product must both be intrinsically appealing to a consumer and the time must be right for purchase. In particular for durable goods, time utility is a function of inter-purchase duration within product category because consumers are unlikely to purchase two items in the same category in close temporal succession. Moreover, purchase data, in contrast to ratings data, is implicit with non-purchases not necessarily indicating dislike. Together, these issues give rise to the positive-unlabeled demand-aware recommendation problem that we pose via joint low-rank tensor completion and product category inter-purchase duration vector estimation. We further relax this problem and propose a highly scalable alternating minimization approach with which we can solve problems with millions of users and millions of items in a single thread. We also show superior prediction accuracies on multiple real-world data sets.\",\n          \"Stochastic Neighbor Embedding (SNE) has shown to be quite promising for data visualization.  Currently, the most popular implementation, t-SNE, is restricted to a particular Student t-distribution as its embedding distribution. Moreover, it uses a gradient descent algorithm that may require users to tune parameters such as the learning step size, momentum, etc., in finding its optimum. In this paper, we propose the Heavy-tailed Symmetric Stochastic Neighbor Embedding (HSSNE) method, which is a generalization of the t-SNE to accommodate various heavy-tailed embedding similarity functions. With this generalization, we are presented with two difficulties.  The first is how to select the best embedding similarity among all heavy-tailed functions and the second is how to optimize the objective function once the heave-tailed function has been selected. Our contributions then are: (1) we point out that various heavy-tailed embedding similarities can be characterized by their negative score functions. Based on this finding, we present a parameterized subset of similarity functions for choosing the best tail-heaviness for HSSNE; (2) we present a fixed-point optimization algorithm that can be applied to all heavy-tailed functions and does not require the user to set any parameters; and (3) we present two empirical studies, one for unsupervised visualization showing that our optimization algorithm runs as fast and as good as the best known t-SNE implementation and the other for semi-supervised visualization showing quantitative superiority using the homogeneity measure as well as qualitative advantage in cluster separation over t-SNE.\",\n          \"Adaptive schemes, where tasks are assigned based on the data collected thus far, are widely used in practical crowdsourcing systems to efficiently allocate the budget. However, existing theoretical analyses of crowdsourcing systems suggest that the gain of adaptive task assignments is minimal. To bridge this gap, we investigate this question under a strictly more general probabilistic model, which has been recently introduced to model practical crowdsourcing data sets. Under this generalized Dawid-Skene model, we characterize the fundamental trade-off between budget and accuracy, and introduce a novel adaptive scheme that matches this fundamental limit. We further quantify the gain of adaptivity, by comparing the trade-off with the one for non-adaptive schemes, and confirm that the gain is significant and can be made arbitrarily large depending on the distribution of the difficulty level of the tasks at hand.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"paper_text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 7237,\n        \"samples\": [\n          \"Statistical Performance of Convex Tensor\\nDecomposition\\nRyota Tomioka?\\nTaiji Suzuki?\\nDepartment of Mathematical Informatics,\\nThe University of Tokyo\\nTokyo 113-8656, Japan\\ntomioka@mist.i.u-tokyo.ac.jp\\ns-taiji@stat.t.u-tokyo.ac.jp\\n\\nKohei Hayashi?\\nGraduate School of Information Science,\\nNara Institute of Science and Technology\\nNara 630-0192, Japan\\nkohei-h@is.naist.jp\\n\\n?\\n\\n?\\n\\nHisashi Kashima?,?\\nBasic Research Programs PRESTO,\\nSynthesis of Knowledge for Information Oriented Society, JST\\nTokyo 102-8666, Japan\\nkashima@mist.i.u-tokyo.ac.jp\\n?\\n\\nAbstract\\nWe analyze the statistical performance of a recently proposed convex tensor decomposition algorithm. Conventionally tensor decomposition has been formulated as non-convex optimization problems, which hindered the analysis of their\\nperformance. We show under some conditions that the mean squared error of\\nthe convex method scales linearly with the quantity we call the normalized rank\\nof the true tensor. The current analysis naturally extends the analysis of convex\\nlow-rank matrix estimation to tensors. Furthermore, we show through numerical\\nexperiments that our theory can precisely predict the scaling behaviour in practice.\\n\\n1 Introduction\\nTensors (multi-way arrays) generalize matrices and naturally represent data having more than two\\nmodalities. For example, multi-variate time-series, for instance, electroencephalography (EEG),\\nrecorded from multiple subjects under various conditions naturally form a tensor. Moreover, in\\ncollaborative ?ltering, users? preferences on products, conventionally represented as a matrix, can\\nbe represented as a tensor when the preferences change over time or context.\\nFor the analysis of tensor data, various models and methods for the low-rank decomposition of\\ntensors have been proposed (see Kolda & Bader [12] for a recent survey). These techniques have\\nrecently become increasingly popular in data-mining [1, 14] and computer vision [25, 26]. Besides\\nthey have proven useful in chemometrics [4], psychometrics [24], and signal processing [20, 7, 8].\\nDespite empirical success, the statistical performance of tensor decomposition algorithms has not\\nbeen fully elucidated. The dif?culty lies in the non-convexity of the conventional tensor decomposition algorithms (e.g., alternating least squares [6]). In addition, studies have revealed many\\ndiscrepancies (see [12]) between matrix rank and tensor rank, which make extension of studies on\\nthe performance of low-rank matrix models (e.g., [9]) challenging.\\nRecently, several authors [21, 10, 13, 23] have focused on the notion of tensor mode-k rank (instead\\nof tensor rank), which is related to the Tucker decomposition [24]. They discovered that regularized\\nestimation based on the Schatten 1-norm, which is a popular technique for recovering low-rank\\nmatrices via convex optimization, can also be applied to tensor decomposition. In particular, the\\n1\\n\\n\\fConvex\\nTucker (exact)\\nOptimization tolerance\\n\\n0\\n\\n10\\n\\n?3\\n\\n10\\n\\n0\\n\\n0.2\\n0.4\\n0.6\\n0.8\\nFraction of observed elements\\n\\n1\\n\\nFigure 1: Result of estimation of rank-(7, 8, 9) tensor of dimensions\\n50???? 50 ? 20 from partial\\n???\\n? ? W ? ??? is plotted against the\\nmeasurements; see [23] for the details. The estimation error ???W\\nF\\nfraction of observed elements m = M/N . Error bars over 10 repetitions are also shown. Convex\\nrefers to the convex tensor decomposition based on the minimization problem (7). Tucker (exact)\\nrefers to the conventional (non-convex) Tucker decomposition [24] at the correct rank. Gray dashed\\nline shows the optimization tolerance 10?3 . The question is how we can predict the point where the\\ngeneralization begins (roughly m = 0.35 in this plot).\\n\\nstudy in [23] showed that there is a clear transition at certain number of samples where the error\\ndrops dramatically from no generalization to perfect generalization (see Figure 1).\\nIn this paper, motivated by the above recent work, we mathematically analyze the performance of\\nconvex tensor decomposition. The new convex formulation for tensor decomposition allows us to\\ngeneralize recent results on Schatten 1-norm-regularized estimation of matrices (see [17, 18, 5, 19]).\\nUnder a general setting we show how the estimation error scales with the mode-k ranks of the true\\ntensor. Furthermore, we analyze the speci?c settings of (i) noisy tensor decomposition and (ii)\\nrandom Gaussian design. In the ?rst setting, we assume that all the elements of a low-rank tensor\\nis observed with noise and the goal is to recover the underlying low-rank structure. This is the most\\ncommon setting a tensor decomposition algorithm is used. In the second setting, we assume that\\nthe unknown tensor is a coef?cient of a tensor-input scalar-output regression problem and the input\\ntensors (design) are randomly given from independent Gaussian distributions. Surprisingly, it turns\\nout that the random Gaussian setting can precisely predict the phase-transition-like behaviour in\\nFigure 1. To the best of our knowledge, this is the ?rst paper that rigorously studies the performance\\nof a tensor decomposition algorithm.\\n\\n2\\n\\nNotation\\n\\nIn this section, we introduce the notations we use in this paper. Moreover, we introduce a H?olderlike inequality (3) and the notion of mode-k decomposability (5), which play central roles in our\\nanalysis.\\nQK\\nLet X ? Rn1 ????nK be a K-way tensor. We denote the number of elements in X by N = k=1 nk .\\n?\\nThe inner product between two tensors ?W, X ? is de?ned as ?W, X ? = vec(W)\\np ), where\\n??? ??? vec(X\\nvec is a vectorization. In addition, we de?ne the Frobenius norm of a tensor ???X ???F = ?X , X ?.\\nQ\\nThe mode-k unfolding X (k) is the nk ? n\\n? \\\\k (?\\nn\\\\k := k? ?=k nk? ) matrix obtained by concatenating\\nthe mode-k ?bers (the vectors obtained by ?xing every index of X but the kth index) of X as column\\nvectors. The mode-k rank of a tensor X , denoted by rankk (X ), is the rank of the mode-k unfolding\\nX (k) (as a matrix). Note that when K = 2 and X is actually a matrix, and X (2) = X (1) ? . We say\\na tensor X is rank (r1 , . . . , rK ) when rk = rankk (X ) for k = 1, . . . , K. Note that the mode-k rank\\ncan be computed in a polynomial time, because it boils down to computing a matrix rank, whereas\\ncomputing tensor rank is NP complete [11]. See [12] for more details.\\nSince for each k, the convex envelope of the mode-k rank is given as the Schatten 1-norm [18]\\n(known as the trace norm [22] or the nuclear norm [3]), it is natural to consider the following\\n2\\n\\n\\f??? ???\\noverlapped Schatten 1-norm ???W ???S of a tensor W ? Rn1 ?????nK (see also [21]):\\n1\\n\\n??? ???\\n???W ???\\n\\nS1\\n\\n=\\n\\nK\\n?\\n1 X?\\n?W (k) ? ,\\nS1\\nK\\n\\n(1)\\n\\nk=1\\n\\nwhere W (k) is the mode-k unfolding of W. Here ? ? ?S1 is the Schatten 1-norm for a matrix\\nXr\\n?W ?S1 =\\n?j (W ),\\nj=1\\n\\nwhere ?j (W ) is the jth largest singular-value of W . The dual norm of the Schatten 1-norm is the\\nSchatten ?-norm (known as the spectral norm) as follows:\\n?X?S? = max ?j (X).\\nj=1,...,r\\n\\nSince the two norms ? ? ?S1 and ? ? ?S? are dual to each other, we have the following inequality:\\n|?W , X?| ? ?W ?S1 ?X?S? ,\\n(2)\\nwhere ?W , X? is the inner product of W and X.\\nThe same inequality holds for the overlapped Schatten 1-norm (1) and its dual norm. The dual norm\\nof the overlapped Schatten 1-norm can be characterized by the following lemma.\\n??? ???\\nLemma 1. The dual norm of the overlapped Schatten 1-norm denoted as ???????S ? is de?ned as the\\n1\\nin?mum of the maximum mode-k spectral norm over the tensors whose average equals the given\\ntensor X as follows:\\n??? ???\\n(k)\\n???X ??? ? =\\nmax ?Y (k) ?S? ,\\ninf\\nS1\\n1\\n(1) +Y (2) +???+Y (K) =X\\nk=1,...,K\\nY\\n(\\n)\\nK\\n(k)\\n\\nwhere Y (k) is the mode-k unfolding of Y (k) . Moreover, the following upper bound on the dual norm\\n??? ???\\n??????? ? is valid:\\nS1\\n\\n??? ???\\n???X ???\\n\\nS1?\\n\\n??? ???\\n1 XK\\n?X (k) ?S? .\\n? ???X ???mean :=\\nk=1\\nK\\n\\n??? ???\\nProof. The ?rst part can be shown by solving the dual of the maximization problem ???X ???S ? :=\\n1\\n??? ???\\nsup ?W, X ? s.t. ???W ???S1 ? 1. The second part is obtained by setting Y (k) = PK K1/c ? X /ck ,\\nwhere ck = ?X (k) ?S? , and using Jensen?s inequality.\\n\\nk? =1\\n\\nk\\n\\nAccording to Lemma 1, we have the ??following\\n? ??? ??? H?\\n?o??lder-like\\n??? inequality\\n??? ??? ???\\n|?W, X ?| ? ???W ???S1 ???X ???S ? ? ???W ???S1 ???X ???mean .\\n\\n(3)\\n??? ??? ??? ???\\nNote that the above bound is tighter than the more intuitive relation | ?W, X ? | ? ???W ???S ???X ???S\\n1\\n?\\n??? ???\\n(???X ???S? := max1,...,K ?X (k) ?S? ), which one might come up as an analogy to the matrix case (2).\\n1\\n\\nFinally, let W ? ? Rn1 ?????nK be the low-rank tensor that we wish to recover. We assume that W ?\\nis rank (r1 , . . . , rK ). Thus, for each k we have\\nW ?(k) = U k S k V k\\n(k = 1, . . . , K),\\nwhere U k ? Rnk ?rk and V k ? Rn? \\\\k ?rk are orthogonal, and S k ? Rrk ?rk is diagonal. Let\\n? ? Rn1 ?????nK be an arbitrary tensor. We de?ne the mode-k orthogonal complement ???k of an\\nunfolding ?(k) ? Rnk ??n\\\\k of ? with respect to the true low-rank tensor W ? as follows:\\n??k\\n\\n???k = (I nk ? U k U k ? )?(k) (I n? \\\\k ? V k V k ? ).\\n\\n(4)\\n\\n:= ?(k) ? ???k is\\nthe true tensor W ?(k) .\\n\\nIn addition\\nthe component having overlapped row/column space with the\\nunfolding of\\nNote that the decomposition ?(k) = ??k + ???k is de?ned for\\neach mode; thus we use subscript k instead of (k).\\nUsing the decomposition de?ned above we have the following equality, which we call mode-k decomposability of the Schatten 1-norm:\\n?W ?(k) + ???k ?S1 = ?W ?(k) ?S1 + ????k ?S1 (k = 1, . . . , K).\\n(5)\\nThe above decomposition is de?ned for each mode and thus it is weaker than the notion of decomposability discussed by Negahban et al. [15].\\n3\\n\\n\\f3\\n\\nTheory\\n\\nIn this section, we ?rst present a deterministic result that holds under a certain choice of regularization constant ?M and an assumption called the restricted strong convexity. Then, we focus on\\nspecial cases to justify the choice of regularization constant and the restricted strong convexity assumption. We analyze the setting of (i) noisy tensor decomposition and (ii) random Gaussian design\\nin Section 3.2 and Section 3.3, respectively.\\n3.1\\n\\nMain result\\n\\nOur goal is to estimate an unknown rank (r1 , . . . , rK ) tensor W ? ? Rn1 ????nK from observations\\nyi = ?Xi , W ? ? + ?i (i = 1, . . . , M ).\\n(6)\\nHere the noise ?i follows the independent zero-mean Gaussian distribution with variance ? 2 .\\nWe employ the regularized empirical risk minimization problem proposed in [21, 10, 13, 23] for the\\nestimation of W as follows:\\n??? ???\\n1\\nminimize\\n?y ? X(W)?22 + ?M ???W ???S1 ,\\n(7)\\nn\\n?????n\\n1\\nK\\n2M\\nW?R\\nwhere y = (y1 , . . . , yM )? is the collection of observations; X : Rn1 ?????nK ? RM is a linear\\noperator that maps W to the M dimensional output vector X(W) = (?X1 , W? , . . . , ?XM , W?) ? ?\\nRM . The Schatten 1-norm term penalizes every mode of W to be jointly low-rank (see Equation (1));\\n?M > 0 is the regularization constant. Accordingly, the solution of the minimization problem (7) is\\ntypically a low-rank tensor when ?M is suf?ciently large. In addition, we denote the adjoint operator\\nPM\\nof X as X? : RM ? Rn1 ?????nK ; that is X? (?) = i=1 ?i Xi ? Rn1 ?????nK .\\n? ? W?\\nThe ?rst step in our analysis is to characterize the particularity of the residual tensor ? := W\\nas in the following lemma.\\n???\\n???\\n? be the solution of the minimization problem (7) with ?M ? 2???X? (?)???\\nLemma 2. Let W\\n/M ,\\nmean\\n? ? W ? , where W ? is the true low-rank tensor. Let ?(k) = ?? + ??? be the\\nand let ? := W\\nk\\nk\\ndecomposition de?ned in Equation (4). Then we have the following inequalities:\\n1. rank(??k ) ? 2rk for each k = 1, . . . , K.\\nPK\\nPK\\n?\\n??\\n2.\\nk=1 ??k ?S1 .\\nk=1 ??k ?S1 ? 3\\nProof. The proof uses the mode-k decomposability (5) and is analogous to that of Lemma 1 in\\n[17].\\nThe second ingredient of our analysis is the restricted strong convexity. Although, ?strong? may\\nsound like a strong assumption, the point is that we require this assumption to hold only for the\\nparticular residual tensor we characterized in Lemma 2. The assumption can be stated as follows.\\nAssumption 1 (Restricted strong convexity). We suppose that there is a positive constant ?(X) such\\nthat the operator X satis?es the inequality\\n??? ???2\\n1\\n?X(?)?22 ??(X)???????F ,\\n(8)\\nM\\nPK\\nfor all ? ? Rn1 ?????nK such that for each k = 1, . . . , K, rank(??k ) ? 2rk and k=1 ????k ?S1 ?\\nPK\\n3 k=1 ???k ?S1 , where ??k and ???k are de?ned through the decomposition (4).\\nNow using the above two ingredients, we are ready to prove the following deterministic guarantee\\non the performance of the estimation procedure (7).\\n???\\n???\\n? be the solution of the minimization problem (7) with ?M ? 2???X? (?)???\\nTheorem 1. Let W\\n/M .\\nmean\\nSuppose that the operator X satis?es the restricted strong convexity condition. Then the following\\nbound is true:\\nPK ?\\n???\\n???\\n? ? W ? ??? ? 32?M k=1 rk .\\n???W\\n(9)\\nF\\n?(X)K\\n4\\n\\n\\f? ? W ? . Combining the fact that the objective value for W\\n?\\nProof. Let ? = W\\n??? ? ???\\n??? is??smaller\\n?\\n??than\\n? ??? that for\\n?\\n?\\n?\\n?\\n?\\n?\\n?\\n?\\n?\\n?\\n?\\n?\\n?\\n?\\n?\\n?\\nW , the H?older-like inequality (3), the triangular inequality W S ? W S ? ?????S , and\\n1\\n1\\n1\\n???\\n???\\nthe assumption ???X? (?)/M ???\\n? ?M /2, we obtain\\nmean\\n\\n??? ???\\n???\\n???\\n??? ???\\n??? ???\\n1\\n(10)\\n?X(?)?22 ? ???X? (?)/M ???mean ???????S1 + ?M ???????S1 ? 2?M ???????S1 .\\n2M\\nNow the left-hand side can be lower-bounded using the restricted strong convexity (8). On the other\\nhand, using Lemma 2, the right-hand side can be upper-bounded as follows:\\n??? ???\\n??? ???\\n??? ???\\n?\\n??????? ? 1 PK (???k ?S1 + ????k ?S1 ) ? 4 PK ???k ?S1 ? 4 ? F PK\\n2rk , (11)\\nk=1\\nk=1\\nk=1\\nK\\nK\\nK\\nS1\\n??? ???\\nwhere the last inequality follows because ???????F = ??(k) ?F for k = 1, . . . , K. Combining inequalities (8), (10), and (11), we obtain our claim (9).\\nNegahban et al. [15] (see also [17]) pointed out that the key properties for establishing a sharp convergence result for a regularized M -estimator is the decomposability of the regularizer and the restricted strong convexity. What we have shown suggests that the weaker mode-k decomposability (5)\\nsuf?ce to obtain the above convergence result for the overlapped Schatten 1-norm (1) regularization.\\n3.2 Noisy Tensor Decomposition\\nIn this subsection, we consider the setting where all the elements are observed (with noise) and the\\ngoal is to recover the underlying low-rank tensor without noise.\\nSince all the elements are observed only once, X is simply a vectorization\\n(M =\\n???\\n??? N ), and the left2\\n? ? W ? ??? . Therefore, the\\n?\\n?\\n?\\n=\\nW\\nhand side of inequality (10)???gives the\\nquantity\\nof\\ninterest\\n?X(?)?\\n2\\nF\\n???\\nremaining task is to bound ???X? (?)???mean as in the following lemma.\\nLemma 3. Suppose\\n???\\n?that\\n?? X : n1 ?? ? ??nK ? N is a vectorization of a tensor. With high probability\\nthe quantity ???X? (?)???mean is concentrated around its mean, which can be bounded as follows:\\nK\\n???\\n???\\n?\\np\\n? X ??\\nE???X? (?)???mean ?\\nnk + n\\n? \\\\k .\\nK\\n\\n(12)\\n\\nk=1\\n\\n???\\n???\\nSetting the regularization constant as ?M = c0 E???X? (?)???mean /N , we obtain the following theorem.\\nTheorem 2. Suppose that X : n1 ?? ? ??nK ? N is a vectorization of a tensor. There are universal\\nconstants c0 and c1 , such that, with high probability, any solution of the minimization problem (7)\\nPK ?\\np\\nwith regularization constant ?M = c0 ? k=1 ( nk + n\\n? \\\\k )/(KN ) satis?es the following bound:\\n?\\n!2 ?\\n!2\\nK\\nK\\nX\\nX\\n???\\n???2\\n??\\n?\\np\\n?\\n1\\n1\\n?\\n2\\n? ? W ??? ? c1 ?\\n???W\\nnk + n\\n? \\\\k\\nrk .\\nF\\nK\\nK\\nk=1\\n\\nk=1\\n\\nProof. Combining Equations (10)?(11) with the fact that X is simply a vectorization and M = N ,\\nwe have\\n?\\n1\\n? ? W ? ?F ? 16 2?M PK ?rk .\\n?W\\nN\\n\\nK\\n\\nk=1\\n\\nSubstituting the choice of regularization constant ?M and squaring both sides, we obtain our claim.?\\nWe can simplify the result of Theorem 2 by noting that n\\n? \\\\k = N/nk ? nk , when the dimenPK ? 2\\n1\\nsions are of the same order. Introducing the notation ?r?1/2 = ( K\\nrk ) and n?1 :=\\nk=1\\n(1/n1 , . . . , 1/nK ), we have\\n???\\n???\\n? ? W ? ???2\\n???W\\n?\\n?\\nF\\n? Op ? 2 ?n?1 ?1/2 ?r?1/2 .\\n(13)\\nN\\nWe call the quantity r? = ?n?1 ?1/2 ?r?1/2 the normalized rank, because r? = r/n when the dimensions are balanced (nk = n and rk = r for all k = 1, . . . , K).\\n5\\n\\n\\f3.3\\n\\nRandom Gaussian Design\\n\\nIn this subsection, we consider the case the elements of the input tensors Xi (i = 1, . . . , M ) in the\\nobservation model (6) are distributed according to independent identical standard Gaussian distributions. We call this setting random Gaussian design.\\n???\\n???\\nFirst we show an upper bound on the norm ???X? (?)???mean , which we use to specify the scaling of\\nthe regularization constant ?M in Theorem 1.\\nLemma 4. Let X : Rn1 ?????nK ? RM be a random Gaussian design. In addition, we assume\\nthat\\n?i is sampled independently from N (0, ? 2 ). Then with high probability the quantity\\n?\\n??? ? the ??noise\\n???X (?)???\\nis concentrated around its mean, which can be bounded as follows:\\nmean\\n???\\n???\\nE???X? (?)???\\n\\nmean\\n\\n?\\nK\\n?\\np\\n? M X ??\\n?\\nnk + n\\n? \\\\k .\\nK\\nk=1\\n\\nNext the following lemma, which is a generalization of a result presented in Negahban and Wainwright [17, Proposition 1], provides a ground for the restricted strong convexity assumption (8).\\nLemma 5. Let X : Rn1 ?????nK ? RM be a random Gaussian design. Then it satis?es\\n?r\\n!\\nr\\nK\\nn\\n? \\\\k ?????? ??????\\n1 X\\n?X(?)?2\\n1 ?????? ??????\\nnk\\n?\\n? F?\\n+\\n? S1 ,\\n?\\n4\\nK\\nM\\nM\\nM\\nk=1\\n\\nwith probability at least 1 ? 2 exp(?N/32).\\nProof. The proof is analogous to that of Proposition 1 in [17] except that we use H?older-like inequality (3) for tensors instead of inequality (2) for matrices.\\nFinally, we obtain the following convergence bound.\\nTheorem 3. Under the random Gaussian design setup, there are universal constants c0 , c1 , and c2\\nPK ?\\nPK ? 2\\np\\n1\\n1\\nsuch that for a sample size M ? c1 ( K\\nn\\n? \\\\k ))2 ( K\\nrk ) , any solution of the\\nk=1 ( nk +\\n?\\nPk=1\\np\\n?\\nK\\nminimization problem (7) with regularization constant ?M = c0 ? k=1 ( nk + n\\n? \\\\k )/(K M )\\nsatis?es the following bound:\\nPK ?\\nPK ? 2\\np\\n1\\n1\\n???\\n???\\n?2 ( K\\nn\\n? \\\\k ))2 ( K\\nk=1 ( nk +\\nk=1 rk )\\n? ? W ? ???2 ? c2\\n???W\\n,\\nF\\nM\\nwith high probability.\\nAgain we can simplify the result of Theorem 3 as follows: for sample size M ? c1 N r? we have\\n?\\n?\\n?1\\n???\\n???\\n? ? W ? ???2 ? Op ? 2 N ?n ?1/2 ?r?1/2 ,\\n???W\\n(14)\\nF\\nM\\nwhere r? = ?n?1 ?1/2 ?r?1/2 is the normalized rank. Note that the condition on the number of\\nsamples M does not depend on the noise variance ? 2 . Therefore in the limit ? 2 ? 0, the bound (14)\\nis suf?ciently small but only valid for sample size M that exceeds c1 N r?, which implies a threshold\\nbehavior as in Figure 1.\\nNote also that in the matrix case (K = 2), r1 = r2 = r and N ?n?1 ?1/2 = O(n1 + n2 ). Therefore\\n? ? W ? ?2 ?\\nwe can restate the above result as for sample size M ? c1 r(n1 + n2 ), we have ?W\\nF\\nOp (r(n1 + n2 )/M ), which is compatible with the result in [17, 18].\\n\\n4\\n\\nExperiments\\n\\nIn this section, we conduct two numerical experiments to con?rm our analysis in Section 3.2 and\\nSection 3.3.\\n6\\n\\n\\f?4\\n\\n3\\n\\nx 10\\n\\n0.03\\n\\nsize=[50 50 20] ?M=0.03/N\\nsize=[50 50 20] ?M=0.33/N\\n\\nsize=[50 50 20] ?M=2.34/N\\n\\nsize=[50 50 20] ? =0.54/N\\n\\n0.025\\n\\nM\\n\\nsize=[100 100 50] ?M=0.66/N\\n\\nsize=[100 100 50] ?M=0.69/N\\n\\nMean squared error\\n\\nMean squared error\\n\\nsize=[50 50 20] ? =6/N\\nM\\n\\nsize=[100 100 50] ?M=0.06/N\\n2\\n\\nsize=[50 50 20] ?M=0.33/N\\n\\nsize=[100 100 50] ? =1.11/N\\nM\\n\\n1\\n\\n0.02\\n\\nsize=[100 100 50] ? =4.5/N\\nM\\n\\nsize=[100 100 50] ?M=12/N\\n0.015\\n\\n0.01\\n\\n0.005\\n\\n0\\n0\\n\\n0.2\\n\\n0.4\\n0.6\\nNormalized rank\\n\\n0.8\\n\\n0\\n0\\n\\n1\\n\\n(a) Small noise (? = 0.01).\\n\\n0.2\\n\\n0.4\\n0.6\\nNormalized rank\\n\\n0.8\\n\\n1\\n\\n(b) Large noise (? = 0.1).\\n\\nFigure 2: Result of noisy tensor decomposition for tensors of size 50 ? 50 ? 20 and 100 ? 100 ? 50.\\n\\n4.1\\n\\nNoisy Tensor Decomposition\\n\\nWe randomly generated low-rank tensors of dimensions n(1) = (50, 50, 20) and n(2) =\\n(100, 100, 50) for various ranks (r1 , . . . , rK ). For a speci?c rank, we generated the true tensor\\nby drawing elements of the r1 ? ? ? ? ? rK ?core tensor? from the standard normal distribution and\\nmultiplying its each mode by an orthonormal factor randomly drawn from the Haar measure. As\\ndescribed in Section 3.2, the observation y consists of all the elements of the original tensor once\\n(M = N ) with additive independent Gaussian noise with variance ? 2 . We used the alternating\\ndirection method of multipliers (ADMM) for ?constraint? approaches described in [23, 10] to solve\\nthe minimization problem (7). The whole experiment was repeated 10 times and averaged.\\n???\\n???\\n? ? W ? ???2 /N is plotted against\\nThe results are shown in Figure 2. The mean squared error ???W\\nF\\nthe normalized rank r? = ?n?1 ?1/2 ?r?1/2 (of the true tensor) de?ned in Equation (13). Since the\\nchoice of the regularization constant ?M only depends on the size of the tensor and not on the ranks\\nof the underlying tensor in Theorem 2, we ?x the regularization constant to some different values\\nand report the dependency of the estimation error on the normalized rank r? of the true tensor.\\nFigure 2(a) shows the result for small noise (? = 0.01) and Figure 2(b) shows the result for large\\n???\\n???\\n? ? W ? ???2 grows linearly\\nnoise (? = 0.1). As predicted by Theorem 2, the squared error ???W\\nF\\nagainst the normalized rank r?. This behaviour is consistently observed not only around the preferred\\nregularization constant value (triangles) but also in the over-?tting case (circles) and the under?tting case (crosses). Moreover, as predicted by Theorem 2, the preferred regularization constant\\nvalue scales linearly and the squared error scales quadratically to the noise standard deviation ?.\\nAs predicted by Lemma 3, the curves for the smaller 50 ? 50 ? 20 tensor and those for the larger\\n100 ? 100 ? 50 tensor seem to agree when the regularization constant\\nis scaled by the factor two.\\np\\nNote that the dominant term in inequality (12) is the second term n\\n? \\\\k , which is roughly scaled by\\nthe factor two from 50 ? 50 ? 20 to 100 ? 100 ? 50.\\n4.2\\n\\nTensor completion from partial observations\\n\\nIn this subsection, we repeat the simulation originally done by Tomioka et al. [23] and demonstrate\\nthat our results in Section 3.3 can precisely predict the empirical scaling behaviour with respect to\\nboth the size and rank of a tensor.\\nWe present results for both matrix completion (K = 2) and tensor completion (K = 3). For\\nthe matrix case, we randomly generated low-rank matrices of dimensions 50 ? 20, 100 ? 40, and\\n250 ? 200. For the tensor case, we randomly generated low-rank tensors of dimensions 50 ? 50 ? 20\\nand 100 ? 100 ? 50. We generated the matrices or tensors as in the previous subsection for various\\nranks. We randomly selected some elements of the true matrix/tensor for training and kept the\\n7\\n\\n\\f1\\n\\n0.8\\n\\n0.8\\n\\n0.6\\n0.4\\nsize=[50 20]\\nsize=[100 40]\\nsize=[250 200]\\n\\n0.2\\n0\\n0\\n\\n0.1\\n0.2\\n0.3\\n0.4\\n0.5\\nNormalized rank ||n?1|| ||r||\\n1/2\\n\\nFraction at Error<=0.01\\n\\nFraction at err<=0.01\\n\\n1\\n\\n0.6\\n0.4\\n0.2\\n0\\n0\\n\\n0.6\\n\\n1/2\\n\\n(a) Matrix completion (K = 2).\\n\\nsize=[50 50 20]\\nsize=[100 100 50]\\n0.2\\n0.4\\n0.6\\nNormalized rank ||n?1||1/2||r||1/2\\n\\n0.8\\n\\n(b) Tensor completion (K = 3).\\n\\nFigure 3: Scaling behaviour of matrix/tensor completion with respect to the size n and the rank r.\\n\\nremaining elements for testing. No observation noise is added. We used the ADMM for ?as a\\nmatrix? and ?constraint? approaches described in [23] to solve the minimization problem (7) for\\nmatrix completion and tensor completion, respectively. Since there is no observation noise, we\\nchose the regularization constant ? ? 0. A single experiment for a speci?c size and rank can be\\nvisualized as in Figure 1.\\n?In\\n?? Figure ?3,\\n??? we plot the minimum fraction of observations m = M/N that achieved error\\n? ? W ??? smaller than 0.01 against the normalized rank r? = ?n?1 ?1/2 ?r?1/2 (of the true ten???W\\nF\\nsor) de?ned in Equation (13). The matrix case is plotted in Figure 3(a) and the tensor case is plotted\\nin Figure 3(b). Each series (blue crosses or red circles) corresponds to different matrix/tensor size\\nand each data-point corresponds to a different core size (rank). We can see that the fraction of observations m = M/N scales linearly against the normalized rank r?, which agrees with the condition\\nM/N ? c1 ?n?1 ?1/2 ?r?1/2 = c1 r? in Theorem 3 (see Equation (14)). The agreement is especially\\ngood for tensor completion (Figure 3(b)), where the two series almost overlap. Interestingly, we\\ncan see that when compared at the same normalized rank, tensor completion is easier than matrix\\ncompletion. For example, when nk = 50 and rk = 10 for each k = 1, . . . , K, the normalized rank\\nis 0.2. From Figure 3, we can see that we only need to see 30% of the entries in the tensor case to\\nachieve error smaller than 0.01, whereas we need about 60% of the entries in the matrix case.\\n\\n5\\n\\nConclusion\\n\\nWe have analyzed the statistical performance of a tensor decomposition algorithm based on the\\noverlapped Schatten 1-norm regularization (7). Numerical experiments show that our theory can\\npredict the empirical scaling behaviour well. The fraction of observation m = M/N at the threshold\\npredicted by our theory is proportional to the quantity we call the normalized rank, which re?nes\\nconjecture (sum of the mode-k ranks) in [23].\\nThere are numerous directions that the current study can be extended. In this paper, we have focused\\non the convergence of the estimation error; it would be meaningful to also analyze the condition for\\nthe consistency of the estimated rank as in [2]. Second, although we have succeeded in predicting\\nthe empirical scaling behaviour, the setting of random Gaussian design does not match the tensor\\ncompletion setting in Section 4.2. In order to analyze the latter setting, the notion of incoherence in\\n[5] or spikiness in [16] might be useful. This might also explain why tensor completion is easier than\\nmatrix completion at the same normalized rank. Moreover, when the target tensor is only low-rank\\nin a certain mode, Schatten 1-norm regularization fails badly (as predicted by the high normalized\\nrank). It would be desirable to analyze the ?Mixture? approach that aims at this case [23]. In\\na broader context, we believe that the current paper could serve as a basis for re-examining the\\nconcept of tensor rank and low-rank approximation of tensors based on convex optimization.\\nAcknowledgments. We would like to thank Franz Kir?aly and Hiroshi Kajino for their valuable\\ncomments and discussions. This work was supported in part by MEXT KAKENHI 22700138,\\n23240019, 23120004, 22700289, and NTT Communication Science Laboratories.\\n8\\n\\n\\fReferences\\n[1] E. Acar and B. Yener. Unsupervised multiway data analysis: A literature survey. IEEE T. Knowl. Data.\\nEn., 21(1):6?20, 2009.\\n[2] F.R. Bach. Consistency of trace norm minimization. J. Mach. Learn. Res., 9:1019?1048, 2008.\\n[3] S. Boyd and L. Vandenberghe. Convex Optimization. Cambridge University Press, 2004.\\n[4] R. Bro. PARAFAC. Tutorial and applications. Chemometr. Intell. Lab., 38(2):149?171, 1997.\\n[5] E. J. Candes and B. Recht. Exact matrix completion via convex optimization. Found. Comput. Math.,\\n9(6):717?772, 2009.\\n[6] J.D. Carroll and J.J. Chang. Analysis of individual differences in multidimensional scaling via an n-way\\ngeneralization of ?Eckart-Young? decomposition. Psychometrika, 35(3):283?319, 1970.\\n[7] P. Comon. Tensor decompositions. In J. G. McWhirter and I. K. Proudler, editors, Mathematics in signal\\nprocessing V. Oxford University Press, 2002.\\n[8] L. De Lathauwer and J. Vandewalle. Dimensionality reduction in higher-order signal processing and\\nrank-(r1 , r2 , . . . , rn ) reduction in multilinear algebra. Linear Algebra Appl., 391:31?55, 2004.\\n[9] K. Fukumizu. Generalization error of linear neural networks in unidenti?able cases. In Algorithmic\\nLearning Theory, pages 51?62. Springer, 1999.\\n[10] S. Gandy, B. Recht, and I. Yamada. Tensor completion and low-n-rank tensor recovery via convex optimization. Inverse Problems, 27:025010, 2011.\\n[11] J. H?astad. Tensor rank is NP-complete. Journal of Algorithms, 11(4):644?654, 1990.\\n[12] T. G. Kolda and B. W. Bader. Tensor decompositions and applications. SIAM Review, 51(3):455?500,\\n2009.\\n[13] J. Liu, P. Musialski, P. Wonka, and J. Ye. Tensor completion for estimating missing values in visual data.\\nIn Prof. ICCV, 2009.\\n[14] M. M?rup. Applications of tensor (multiway array) factorizations and decompositions in data mining.\\nWiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery, 1(1):24?40, 2011.\\n[15] S. Negahban, P. Ravikumar, M. Wainwright, and B. Yu. A uni?ed framework for high-dimensional\\nanalysis of m-estimators with decomposable regularizers. In Y. Bengio, D. Schuurmans, J. Lafferty,\\nC. K. I. Williams, and A. Culotta, editors, Advances in NIPS 22, pages 1348?1356. 2009.\\n[16] S. Negahban and M.J. Wainwright. Restricted strong convexity and weighted matrix completion: Optimal\\nbounds with noise. Technical report, arXiv:1009.2118, 2010.\\n[17] S. Negahban and M.J. Wainwright. Estimation of (near) low-rank matrices with noise and highdimensional scaling. Ann. Statist., 39(2), 2011.\\n[18] B. Recht, M. Fazel, and P.A. Parrilo. Guaranteed minimum-rank solutions of linear matrix equations via\\nnuclear norm minimization. SIAM Review, 52(3):471?501, 2010.\\n[19] A. Rohde and A.B. Tsybakov.\\n39(2):887?930, 2011.\\n\\nEstimation of high-dimensional low-rank matrices.\\n\\nAnn. Statist.,\\n\\n[20] N.D. Sidiropoulos, R. Bro, and G.B. Giannakis. Parallel factor analysis in sensor array processing. IEEE\\nT. Signal Proces., 48(8):2377?2388, 2000.\\n[21] M. Signoretto, L. De Lathauwer, and J.A.K. Suykens. Nuclear norms for tensors and their use for convex\\nmultilinear estimation. Technical Report 10-186, ESAT-SISTA, K.U.Leuven, 2010.\\n[22] N. Srebro, J. D. M. Rennie, and T. S. Jaakkola. Maximum-margin matrix factorization. In Lawrence K.\\nSaul, Yair Weiss, and L?eon Bottou, editors, Advances in NIPS 17, pages 1329?1336. MIT Press, Cambridge, MA, 2005.\\n[23] R. Tomioka, K. Hayashi, and H. Kashima. Estimation of low-rank tensors via convex optimization.\\nTechnical report, arXiv:1010.0789, 2011.\\n[24] L. R. Tucker. Some mathematical notes on three-mode factor analysis. Psychometrika, 31(3):279?311,\\n1966.\\n[25] M. Vasilescu and D. Terzopoulos. Multilinear analysis of image ensembles: Tensorfaces. Computer\\nVision?ECCV 2002, pages 447?460, 2002.\\n[26] H. Wang and N. Ahuja. Facial expression decomposition. In Proc. 9th ICCV, pages 958 ? 965, 2003.\\n\\n9\\n\\n\\f\",\n          \"Effects of Spatial and Temporal Contiguity on\\nthe Acquisition of Spatial Information\\n\\nThea B. Ghiselli-Crippa and Paul W. Munro\\nDepartment of Information Science and Telecommunications\\nUniversity of Pittsburgh\\nPittsburgh, PA 15260\\ntbgst@sis.pitt.edu, munro@sis.pitt.edu\\n\\nAbstract\\nSpatial information comes in two forms: direct spatial information (for\\nexample, retinal position) and indirect temporal contiguity information,\\nsince objects encountered sequentially are in general spatially close. The\\nacquisition of spatial information by a neural network is investigated\\nhere. Given a spatial layout of several objects, networks are trained on a\\nprediction task. Networks using temporal sequences with no direct spatial information are found to develop internal representations that show\\ndistances correlated with distances in the external layout. The influence\\nof spatial information is analyzed by providing direct spatial information\\nto the system during training that is either consistent with the layout or\\ninconsistent with it. This approach allows examination of the relative\\ncontributions of spatial and temporal contiguity.\\n\\n1 Introduction\\nSpatial information is acquired by a process of exploration that is fundamentally temporal, whether it be on a small scale, such as scanning a picture, or on a larger one, such as\\nphysically navigating through a building, a neighborhood, or a city. Continuous scanning\\nof an environment causes locations that are spatially close to have a tendency to occur in\\ntemporal proximity to one another. Thus, a temporal associative mechanism (such as a\\nHebb rule) can be used in conjunction with continuous exploration to capture the spatial\\nstructure of the environment [1]. However, the actual process of building a cognitive map\\nneed not rely solely on temporal associations, since some spatial information is encoded in\\nthe sensory array (position on the retina and proprioceptive feedback). Laboratory studies\\nshow different types of interaction between the relative contributions of temporal and spatial contiguities to the formation of an internal representation of space. While Clayton and\\nHabibi's [2] series of recognition priming experiments indicates that priming is controlled\\nonly by temporal associations, in the work of McNamara et al. [3] priming in recognition is observed only when space and time are both contiguous. In addition, Curiel and\\nRadvansky's [4] work shows that the effects of spatial and temporal contiguity depend on\\nwhether location or identity information is emphasized during learning. Moreover, other\\nexperiments ([3]) also show how the effects clearly depend on the task and can be quite\\ndifferent if an explicitly spatial task is used (e.g., additive effects in location judgments).\\n\\n\\fT. B. Ghiselli-Crippa and P W. Munro\\n\\n18\\n\\nlabels\\n\\nlabels\\n\\nlabels\\n(A coeff.)\\n\\nlabels\\n\\nlabels\\n\\ncoordinates\\n\\ncoordinates\\n(B coeff.)\\n\\nlabels\\n\\nFigure 1: Network architectures: temporal-only network (left); spatio-temporal network\\nwith spatial units part of the input representation (center); spatio-temporal network with\\nspatial units part of the output representation (right) .\\n\\n2 Network architectures\\nThe goal of the work presented in this paper is to study the structure of the internal representations that emerge from the integration of temporal and spatial associations. An\\nencoder-like network architecture is used (see Figure 1), with a set of N input units and a\\nset of N output units representing N nodes on a 2-dimensional graph. A set of H units is\\nused for the hidden layer. To include space in the learning process, additional spatial units\\nare included in the network architecture. These units provide a representation of the spatial\\ninformation directly available during the learning/scanning process. In the simulations described in this paper, two units are used and are chosen to represent the (x, y) coordinates of\\nthe nodes in the graph . The spatial units can be included as part of the input representation\\nor as part of the output representation (see Figure 1, center and right panels): both choices\\nare used in the experiments, to investigate whether the spatial information could better benefit training as an input or as an output [5]. In the second case, the relative contribution of\\nthe spatial information can be directly manipulated by introducing weighting factors in the\\ncost function being minimized. A two-term cost function is used, with a cross-entropy term\\nfor the N label units and a squared error term for the 2 coordinate units,\\n\\nri indicates the actual output of unit i and ti its desired output. The relative influence of\\n\\nthe spatial information is controlled by the coefficients A and B.\\n\\n3\\n\\nLearning tasks\\n\\nThe left panel of Figure 2 shows an example of the type of layout used; the effective\\nlayout used in the study consists of N = 28 nodes. For each node, a set of neighboring\\nnodes is defined, chosen on the basis of how an observer might scan the layout to learn the\\nnode labels and their (spatial) relationships; in Figure 2, the neighborhood relationships are\\nrepresented by lines connecting neighboring nodes. From any node in the layout, the only\\nallowed transitions are those to a neighbor, thus defining the set of node pairs used to train\\nthe network (66 pairs out of C(28, 2) = 378 possible pairs). In addition, the probability\\nof occurrence of a particular transition is computed as a function of the distance to the\\ncorresponding neighbor. It is then possible to generate a sequence of visits to the network\\nnodes, aimed at replicating the scanning process of a human observer studying the layout.\\n\\n\\f19\\n\\nSpatiotemporal Contiguity Effects on Spatial Information Acquisition\\n\\neraser\\n\\nknife\\n\\ncup\\n\\ncoin\\n\\neraser\\n\\nbutton\\n\\nFigure 2: Example of a layout (left) and its permuted version (right). Links represent\\nallowed transitions. A larger layout of 28 units was used in the simulations.\\n\\nThe basic learning task is similar to the grammar learning task of Servan-Schreiber et al.\\n[6] and to the neighborhood mapping task described in [1] and is used to associate each of\\nthe N nodes on the graph and its (x, y) coordinates with the probability distribution of the\\ntransitions to its neighboring nodes. The mapping can be learned directly, by associating\\neach node with the probability distribution of the transitions to all its neighbors: in this\\ncase, batch learning is used as the method of choice for learning the mapping. On the\\nother hand, the mapping can be learned indirectly, by associating each node with itself\\nand one of its neighbors, with online learning being the method of choice in this case;\\nthe neighbor chosen at each iteration is defined by the sequence of visits generated on\\nthe basis of the transition probabilities. Batch learning was chosen because it generally\\nconverges more smoothly and more quickly than online learning and gives qualitatively\\nsimilar results. While the task and network architecture described in [1] allowed only\\nfor temporal association learning, in this study both temporal and spatial associations are\\nlearned simultaneously, thanks to the presence of the spatial units. However, the temporalonly (T-only) case, which has no spatial units, is included in the simulations performed\\nfor this study, to provide a benchmark for the evaluation of the results obtained with the\\nspatio-temporal (S- T) networks.\\nThe task described above allows the network to learn neighborhood relationships for which\\nspatial and temporal associations provide consistent information, that is, nodes experienced\\ncontiguously in time (as defined by the sequence) are also contiguous in space (being spatial neighbors). To tease apart the relative contributions of space and time, the task is kept\\nthe same, but the data employed for training the network is modified: the same layout is\\nused to generate the temporal sequence, but the x , y coordinates of the nodes are randomly\\npermuted (see right panel of Figure 2). If the permuted layout is then scanned following the\\nsame sequence of node visits used in the original version, the net effect is that the temporal\\nassociations remain the same, but the spatial associations change so that temporally neighboring nodes can now be spatially close or distant: the spatial associations are no longer\\nconsistent with the temporal associations. As Figure 4 illustrates, the training pairs (filled\\ncircles) all correspond to short distances in the original layout, but can have a distance\\nanywhere in the allowable range in the permuted layout. Since the temporal and spatial\\ndistances were consistent in the original layout, the original spatial distance can be used\\nas an indicator of temporal distance and Figure 4 can be interpreted as a plot of temporal\\ndistance vs. spatial distance for the permuted layout.\\nThe simulations described in the following include three experimental conditions: temporal\\nonly (no direct spatial information available); space and time consistent (the spatial coordinates and the temporal sequence are from the same layout); space and time inconsistent\\n(the spatial coordinates and the temporal sequence are from different layouts).\\n\\n\\fT. B. Ghise/li-Crippa and P. W. Munro\\n\\n20\\n\\nHidden unit representations are compared using Euclidean distance (cosine and inner product measures give consistent results); the internal representation distances are also used to\\ncompute their correlation with Euclidean distances between nodes in the layout (original\\nand permuted). The correlations increase with the number of hidden units for values of\\nH between 5 and 10 and then gradually taper off for values greater than 10. The results\\npresented in the remainder of the paper all pertain to networks trained with H = 20 and\\nwith hidden units using a tanh transfer function; all the results pertaining to S-T networks\\nrefer to networks with 2 spatial output units and cost function coefficients A = 0.625 and\\nB = 6.25.\\n\\n4 Results\\nFigure 3 provides a combined view of the results from all three experiments. The left panel\\nillustrates the evolution of the correlation between internal representation distances and\\nlayout (original and permuted) distances. The right panel shows the distributions of the\\ncorrelations at the end of training (1000 epochs). The first general result is that, when spatial information is available and consistent with the temporal information (original layout),\\nthe correlation between hidden unit distances and layout distances is consistently better\\nthan the correlation obtained in the case of temporal associations alone. The second general result is that, when spatial information is available but not consistent with the temporal\\ninformation (permuted layout), the correlation between hidden unit distances and original\\nlayout distances (which represent temporal distances) is similar to that obtained in the case\\nof temporal associations alone, except for the initial transient. When the correlation is computed with respect to the permuted layout distances, its value peaks early during training\\nand then decreases rapidly, to reach an asymptotic value well below the other three cases.\\nThis behavior is illustrated in the box plots in the right panel of Figure 3, which report the\\ndistribution of correlation values at the end of training.\\n\\n4.1\\n\\nTemporal-only vs. spatio-temporal\\n\\nAs a first step in this study, the effects of adding spatial information to the basic temporal\\nassociations used to train the network can be examined. Since the learning task is the same\\nfor both the T-only and the S-T networks except for the absence or presence of spatial\\ninformation during training, the differences observed can be attributed to the additional\\nspatial information available to the S-T networks. The higher correlation between internal\\nrepresentation distances and original layout distances obtained when spatial information is\\n\\n0\\n\\n~\\n\\n.,\\n\\n8\\n\\n.,\\n\\nS and T CO\\\"Isistent\\n\\n0\\n\\n.\\n\\n0\\n\\n.\\n\\nT-o\\\"\\nSand T InCOnsistent\\n\\n0\\n\\ni:i\\n\\n-==~\\n\\n0\\n\\n(corr with T distance)\\n\\nii\\n\\n...\\n\\n?8 \\\"\\n\\n\\\"0\\n\\n0\\n\\n=s:\\n...........\\nE:2\\n\\nS and T Ir'ICOOSlStent\\n(corr. Wflh S distance)\\n\\n'\\\"ci\\n\\n~\\n\\n--'----'\\n\\nN\\n\\n0\\n\\n0\\n0\\n\\n0\\n0\\n\\n200\\n\\n400\\n600\\nOllnber 01 epochs\\n\\n800\\n\\n1000\\n\\nSandT\\n\\ncon_atent\\n\\nT-only\\n\\nSandT\\nSandT\\nInconsistent\\nineon.stant\\n(corr \\\" th T ast ) (corr wth 5 dst )\\n\\nFigure 3: Evolution of correlation during training (0 - 1000 epochs) (left). Distributions of\\ncorrelations at the end of training (1000 epochs) (right).\\n\\n\\fSpatiotemporal Contiguity Effects on Spatial Information Acquisition\\n\\n-\\n\\n21\\n\\nN\\n\\ndHU = 0.6 + 3.4d T + 0.3ds - 2.1( dT)2 + 0.4( d S )2 - 0.4d T ds\\n\\n0\\n\\n.,\\n\\n25\\n\\n0\\n\\n\\\",\\nE\\n~\\n\\n'\\\"\\n0\\n\\n15\\n\\n...\\n0\\n\\n05\\nN\\n\\n0\\n\\n15\\n0\\n0\\n\\n00\\n\\n02\\n\\n04\\n\\n08\\n\\n10\\n\\n12\\n\\nFigure 4: Distances in the original layout\\n(x) vs_ distances in the permuted layout\\n(y)_ The 66 training pairs are identified by\\nfilled circles_\\n\\n\\\"\\nFigure 5: Similarities (Euclidean distances)\\nbetween internal representations developed\\nby a S-T network (after 300 epochs)_ Figure\\n4 projects the data points onto the x, y plane_\\n\\navailable (see Figure 3) is apparent also when the evolution of the internal representations\\nis examined_ As Figure 6 illustrates, the presence of spatial information results in better\\ngeneralization for the pattern pairs outside the training set While the distances between\\ntraining pairs are mapped to similar distances in hidden unit space for both the T-only and\\nthe S-T networks, the T-only network tends to cluster the non-training pairs into a narrow\\nband of distances in hidden unit space. In the case of the S-T network instead, the hidden\\nunit distances between non-training pairs are spread out over a wider range and tend to\\nreflect the original layout distances.\\n4.2\\n\\nPermuted layout\\n\\nAs described above, with the permuted layout it is possible to decouple the spatial and\\ntemporal contributions and therefore study the effects of each. A comprehensive view of\\nthe results at a particular point during training (300 epochs) is presented in Figure 5, where\\nthe x, y plane represents temporal distance vs. spatial distance (see also Figure 4) and the z\\naxis represents the similarity between hidden unit representations. The figure also includes\\na quadratic regression surface fitted to the data points. The coefficients in the equation of\\nthe surface provide a quantitative measure of the relative contributions of spatial (ds) and\\ntemporal distances (dT ) to the similarity between hidden unit representations (d HU ):\\n(2)\\n\\nIn general, after the transient observed in early training (see Figure 3), the largest and most\\nsignificant coefficients are found for dT and (dT?, indicating a stronger dependence of\\ndHU on temporal distance than on spatial distance.\\nThe results illustrated in Figure 5 represent the situation at a particular point during training\\n(300 epochs). Similar plots can be generated for different points during training, to study\\nthe evolution of the internal representations. A different view of the evolution process is\\nprovided by Figure 7, in which the data points are projected onto the x,Z plane (top panel)\\nand the y,z plane (bottom panel) at four different times during training. In the top panel,\\n\\n14\\n\\n\\fT. B. Ghiselli-Crippa and P W Munro\\n\\n22\\n\\n,.. ,..\\n.. roo\\n:::\\n\\n~\\n_\\n\\n0\\n\\n?\\n\\nN\\n\\n~ ~\\n~ ~\\n\\n~\\n\\n-. .. -\\n\\n:::\\n~\\n\\n~\\n\\n02\\n\\n\\\"\\n\\n06\\n\\nO.\\n\\n\\\"_d\\n\\n\\\"\\n\\n12\\n\\n.\\n\\n,,\\n\\n~'\\n\\n~ :\\n~\\n\\n~\\n~,\\n\\n02\\n\\nos\\n\\n\\\"-'\\n\\n..\\n\\n'\\n\\n02\\n\\n.. .\\n06\\n\\n-\\n\\n,\\n\\ntP\\n\\n.\\n\\nDO\\n\\n0\\n\\n, ,\\n.I'\\n\\n~\\n\\n12\\n\\n.',\\n\\n00\\n\\n02\\n\\n\\\" \\\"-'\\n06\\n\\n.\\\"\\n\\n02\\n\\n~\\n\\n~\\n\\n.~.\\n','\\n\\n~\\n\\n.. .. \\\" \\\"\\n\\n~\\n\\n06\\n\\n00\\n\\n02\\n\\n\\\"_d\\n\\n_\\n\\n0\\n\\n?\\n\\nN\\n\\n~\\n\\n:\\n\\n~,\\n\\n~ ~\\n\\n..\\n\\nf/Po\\n\\n<P\\n\\n\\\"\\n\\ne,\\n\\n.\\n\\nDO\\n\\n.:.\\n\\n~\\n\\n00\\n\\n02\\n\\n\\\"\\n\\n.. ..\\n\\n\\\"-'\\n\\n10\\n\\n12\\n\\n.. .. .. \\\"\\n\\n12\\n\\n\\\" _d\\n\\n:::\\n\\n~\\n,\\n\\n~\\n12\\n\\n0\\nN\\n\\n~ ~\\n\\n',~-,\\n\\n00\\n\\n~\\n\\n~\\n\\n~\\n_\\n?\\n\\n~\\n\\n~\\n\\n12\\n\\n\\\"\\n\\n0\\n\\n00\\n\\n~\\n\\n:::\\n\\ng\\n10\\n\\n~\\n\\n~\\n\\n~ ~\\n\\nO.\\n\\n,\\n\\n:::\\n\\n;; ~\\n\\n.~\\n00\\n\\n00\\n\\n~.\\n\\n~\\n\\n,\\n\\n\\\"_d\\n\\n,\\n\\n:; ~\\n\\n~\\n\\n~\\n\\n~\\n\\n00\\n\\n~\\n\\n~\\n\\n~\\n\\n~\\n\\ni\\n\\n~\\n\\n~\\n\\n~,\\n\\n:::\\n\\n~\\n\\n~\\n\\n~\\n_\\n\\n0\\n\\n?\\n\\nN\\n\\n~\\n\\n~\\n\\n~\\n\\n~\\n\\no\\n\\n,.~,o\\n\\n: s\\n\\nrIP 0\\n\\n00\\n\\n0\\n\\n?\\n\\n','\\n\\n00\\n\\n02\\n\\n\\\"\\n\\nO.\\n\\n\\\"-'\\n\\no.\\n\\n\\\"\\n\\n12\\n\\nFigure 6: Internal representation distances vs. original layout distances: S-T network (top)\\nvs. T-only network (bottom). The training pairs are identified by filled circles. The presence\\nof spatial information results in better generalization for the pairs outside the training set.\\nthe internal representation distances are plotted as a function of temporal distance (i.e., the\\nspatial distance from the original layout), while in the bottom panel they are plotted as a\\nfunction of spatial distance (from the permuted layout). The higher asymptotic correlation\\nbetween internal representation distances and temporal distances, as opposed to spatial\\ndistances (see Figure 3), is apparent also from the examination of the evolutionary plots,\\nwhich show an asymptotic behavior with respect to temporal distances (see Figure 7, top\\npanel) very similar to the T-only case (see Figure 6, bottom panel).\\n\\n5 Discussion\\nThe first general conclusion that can be drawn from the examination of the results described\\nin the previous section is that, when the spatial information is available and consistent with\\nthe temporal information (original layout), the similarity structure of the hidden unit representations is closer to the structure of the original layout than that obtained by using\\ntemporal associations alone. The second general conclusion is that, when the spatial information is available but not consistent with the temporal information (permuted layout),\\nthe similarity structure of the hidden unit representations seems to correspond to temporal\\nmore than spatial proximity. Figures 5 and 7 both indicate that temporal associations take\\nprecedence over spatial associations. This result is in agreement with the results described\\nin [1], showing how temporal associations (plus some high-level constraints) significantly\\ncontribute to the internal representation of global spatial information. However, spatial information certainly is very beneficial to the (temporal) acquisition of a layout, as proven by\\nthe results obtained with the S-T network vs. the T-only network.\\nIn terms of the model presented in this paper, the results illustrated in Figures 5 and 7 can\\nbe compared with the experimental data reported for recognition priming ([2], [3], [4]),\\nwith distance between internal representations corresponding to reaction time. The results\\nof our model indicate that distances in both the spatially far and spatially close condition\\nappear to be consistently shorter for the training pairs (temporally close) than for the nontraining pairs (temporally distant), highlighting a strong temporal effect consistent with the\\ndata reported in [2] and [4] (for spatially far pairs) and in [3] (only for the spatially close\\n\\n\\fSpatiotemporal Contiguity Effects on Spatial Information Acquisition\\n\\n~~\\n\\n0_\\n\\nri\\n\\n; -~-'\\n~. ~~.. .\\nSl\\n...........\\n0\\n\\n\\\" ...... .\\nj!I!A\\n\\n..\\n,.\\n\\n0\\n\\n,.\\n\\n~\\n\\n~\\n\\n~\\n\\n23\\n\\n\\\\\\n\\n0\\n\\n?\\n\\nlfIiiIo\\n\\n'0'\\n\\n110\\n\\n0\\n\\n~'--_ _ _ _ _-.J\\n\\n00\\n\\n02\\n\\nO.\\n\\n01\\n\\n01\\n\\n10\\n\\n12\\n\\n00\\n\\n02\\n\\nO.\\n\\n~\\n\\n01\\n\\n01\\n\\n10\\n\\n12\\n\\n00\\n\\n02\\n\\n0.4\\n\\n01\\n\\n01\\n\\n10\\n\\n12\\n\\n02\\n\\n0\\\"\\n\\n01\\nIn_d (S)\\n\\n01\\n\\n'0\\n\\n12\\n\\n~l.-\\n\\n00\\n\\n0.2\\n\\no.\\n\\not\\n.._d(S)\\n\\n02\\n\\nO.\\n\\n01\\n\\n10\\n\\n12\\n\\n0.0\\n\\n02\\n\\n04\\n\\n08\\n...u:I (S)\\n\\n01\\n\\noa\\n\\n10\\n\\n12\\n\\nIn_den\\n\\nL..-_ _ _ _- . l\\n00\\n\\n0 0\\n\\nl'I_d (T)\\n\\nIn_d(TI\\n\\nIn _d (T}\\n\\nall\\n\\n10\\n\\n12\\n\\n00\\n\\n_ _ _ _ _-.J\\n02\\n\\nO.\\n\\n06\\n\\noa\\n\\n10\\n\\n12\\n\\n!rUi (S)\\n\\nFigure 7: Internal representation distances vs. temporal distances (top) and vs. spatial\\ndistances (bottom) for a S-T network (permuted layout). The training pairs are identified\\nby filled circles. The asymptotic behavior with respect to temporal distances (top panel) is\\nsimilar to the T-only condition. The bottom panel indicates a weak dependence on spatial\\ndistances.\\ncase). For the training pairs (temporally close), slightly shorter distances are obtained for\\nspatially close pairs vs. spatially far pairs; this result does not provide support for the\\nexperimental data reported in either [3] (strong spatial effect) or [2] (no spatial effect).\\nFor the non-training pairs (temporally distant), long distances are found throughout, with\\nno strong dependence on spatial distance; this effect is consistent with all the reported\\nexperimental data. Further simulations and statistical analyses are necessary for a more\\nconclusive comparison with the experimental data.\\nReferences\\n[1] Ghiselli-Crippa, TB. & Munro, P.w. (1994). Emergence of global structure from local associations. In J.D. Cowan, G. Tesauro, & J. Alspector (Eds.), Advances in Neural Information Processing\\nSystems 6, pp. 1101-1108. San Francisco, CA: Morgan Kaufmann.\\n[2] Clayton, K.N. & Habibi, A. (1991). The contribution of temporal contiguity to the spatial priming\\neffect. Journal of Experimental Psychology: Learning. Memory. and Cognition 17:263-271.\\n[3] McNamara, TP., Halpin. J.A. & Hardy, J.K. (1992). Spatial and temporal contributions to the\\nstructure of spatial memory. Journal of Experimental Psychology: Learning. Memory. and Cognition\\n18:555-564.\\n[4] Curiel, J.M. & Radvansky, G.A. (1998). Mental organization of maps. Journal of Experimental\\nPsychology: Learning. Memory. and Cognition 24:202-214.\\n[5] Caruana, R. & de Sa, VR. (1997). Promoting poor features to supervisors: Some inputs work\\nbetter as outputs . In M.e. Mozer, M.I. Jordan, & T Petsche (Eds.), Advances in Neural Information\\nProcessing Systems 9, pp. 389-395. Cambridge, MA: MIT Press.\\n[6] Servan-Schreiber, D., Cleeremans, A. & McClelland, J.L. (1989). Learning sequential structure\\nin simple recurrent networks. In D.S. Touretzky (Ed.), Advances in Neural Information Processing\\nSystems 1, pp. 643-652. San Mateo, CA: Morgan Kaufmann.\\n\\n\\fNeural Representation of Multi-Dimensional\\nStimuli\\n\\nChristian W. Eurich, Stefan D. Wilke and Helmut Schwegler\\nInstitut fUr Theoretische Physik\\nUniversitat Bremen, Germany\\n(eurich,swilke,schwegler)@physik.uni-bremen.de\\n\\nAbstract\\nThe encoding accuracy of a population of stochastically spiking neurons\\nis studied for different distributions of their tuning widths. The situation\\nof identical radially symmetric receptive fields for all neurons, which\\nis usually considered in the literature, turns out to be disadvantageous\\nfrom an information-theoretic point of view. Both a variability of tuning widths and a fragmentation of the neural population into specialized\\nsubpopulations improve the encoding accuracy.\\n\\n1 Introduction\\nThe topic of neuronal tuning properties and their functional significance has focused much\\nattention in the last decades. However, neither empirical findings nor theoretical considerations have yielded a unified picture of optimal neural encoding strategies given a sensory\\nor motor task. More specifically, the question as to whether narrow tuning or broad tuning\\nis advantageous for the representation of a set of stimulus features is still being discussed.\\nEmpirically, both situations are encountered: small receptive fields whose diameter is less\\nthan one degree can, for example, be found in the human retina [7] , and large receptive\\nfields up to 180 0 in diameter occur in the visual system of tongue-projecting salamanders\\n[10]. On the theoretical side, arguments have been put forward for small [8] as well as for\\nlarge [5, 1,9, 3, 13] receptive fields.\\nIn the last years, several approaches have been made to calculate the encoding accuracy\\nof a neural population as a function of receptive field size [5, 1,9,3, 13]. It has turned\\nout that for a firing rate coding, large receptive fields are advantageous provided that D 2:\\n3 stimulus features are encoded [9, 13]. For binary neurons, large receptive fields are\\nadvantageous also for D = 2 [5,3].\\nHowever, so far only radially symmetric tuning curves have been considered. For neural\\npopulations which lack this symmetry, the situation may be very different. Here we study\\nthe encoding accuracy of a popUlation of stochastically spiking neurons. A Fisher information analysis performed on different distributions of tunings widths will indeed reveal a\\nmuch more detailed picture of neural encoding strategies.\\n\\n\\fC. W. Eurich. S. D. Wilke and H. Schwegler\\n\\nJ J6\\n\\n2 Model\\nConsider a D-dimensional stimulus space, X. A stimulus is characterized by a position\\nx\\n(Xl, ... , XD) E X, where the value of feature i, Xi (i\\n1, ... , D), is measured\\nrelative to the total range of values in the i-th dimension such that it is dimensionless.\\nInformation about the stimulus is encoded by a popUlation of N stochastically spiking\\nneurons. They are assumed to have independent spike generation mechanisms such that the\\njoint probability distribution for observing n = (n(l), ... ,n(k), ... ,n(N?) spikes within a\\ntime interval T, Ps(n; x), can be written in the form\\n\\n=\\n\\n=\\n\\nN\\n\\nPs(n;x) =\\n\\nII\\n\\nps(k) (n(k);\\n\\nx),\\n\\n(1)\\n\\nk=l\\nwhere Ps(k) (n(k); x) is the single-neuron probability distribution of the number of observed\\nspikes given the stimulus at position x. Note that (1) does not exclude a correlation of the\\nneural firing rates, i.e., the neurons may have common input or even share the same tuning\\nfunction.\\nThe firing rates depend on the stimulus via the local values of the tuning functions, such that\\nx) can be written in the form Ps(k) (n(k); x) = S (n(k), j(k) (x), T), where the\\ntuning function of neuron k, j(k) (x), gives its mean firing rate in response to the stimulus\\nat position x. We assume here a form of the tuning function that is not necessarily radially\\nsymmetric,\\nPs(k) (n(k);\\n\\nf(') (x)\\n\\n= F4>\\n\\n(t\\n\\n(Xi\\n\\n~~r) )2) =, F? ( e( ')2) ,\\n\\n(2)\\n\\nwhere e(k) = (c~k), ... , c};?) is the center of the tuning curve of neuron k, O'~k) is its\\ntuning width in the i-th dimension, k )2 := (Xi - c~k?)2/O'ik)2 for i = 1, ... ,D, and\\n~(k)2 := ~~k)2 + ... + ~~)2. F > 0 denotes the maximal firing rate of the neurons, which\\nrequires that maxz~o fj>(z) = 1.\\n\\nd\\n\\nWe assume that the tuning widths O't), . .. ,O'~) of each neuron k are drawn from a distribution PO' (0'1, ... ,O'D). For a population oftuning functions with centers e(l), ... , e(N), a\\ndensity 1}(x) is introduced according to 1}(x) := L:~=l 8(x - e(k?).\\nThe encoding accuracy can be quantified by the Fisher information matrix, J, which is\\ndefined as\\n(3)\\n\\nwhere E[ . ..J denotes the expectation value over the probability distribution P(n; x) [2].\\nThe Fisher information yields a lower bound on the expected error of an unbiased estimator\\nthat retrieves the stimulus x from the noisy neural activity (Cramer-Rao inequality) [2]. The\\nminimal estimation error for the i-th feature Xi, ti,min, is given by t;,min = (J - 1 )ii which\\nreduces to t;,min = 1/ Jii(X) if J is diagonal.\\nWe shall now derive a general expression for the popUlation Fisher information. In the\\nnext chapter, several cases and their consequences for neural encoding strategies will be\\ndiscussed.\\nFor model neuron (k), the Fisher information (3) reduces to\\n(k)\\n\\nJ ij\\n\\n.\\n\\n(k)\\n\\n(X'O'I\\n\\n(k) _\\n\\\"\\\"'O'D) -\\n\\n1\\n\\n(k)\\nO'i\\n\\n(k)Aq..\\nO'j\\n\\n(\\n\\n~\\n\\n(k)2\\n\\n,F,T\\n\\n)\\n\\n(k) (k)\\n\\n~i ~j\\n\\n,\\n\\n(4)\\n\\n\\f117\\n\\nNeural Representation of Multi-Dimensional Stimuli\\n\\nwhere the dependence on the tuning widths is indicated by the list of arguments. The\\nfunction A.p depends on the shape of the tuning function and is given in [13]. The independence assumption (1) implies that the population Fisher information is the sum of\\n. d??d\\nI\\n\\\",N J(k)(\\n(k)\\n(k)) . U7\\nt he contn?b?\\nutlOns 0 f the III\\nIVI ua neurons, L.Jk=1 ij x; 0\\\"1 , ... ,0\\\"D\\nne now define\\na population Fisher information which is averaged over the distribution of tuning widths\\nPt:T(0\\\"1, . .. ,O\\\"D):\\nN\\n\\n(Jij (x)) 17 =\\n\\nL / d0\\\"1 . .. dO\\\"D Pt:T(0\\\"1,? .. , O\\\"D) Ji~k) (x; 0\\\"1, ? .. , O\\\"D) .\\n\\n(5)\\n\\nk= 1\\n\\nIntroducing the density of tuning curves, 1J(x), into (5) and assuming a constant distribution, 1J(x) == 1J == const., one obtains the result that the population Fisher information\\nbecomes independentofx and that the off-diagonal elements of J vanish [13]. The average\\npopulation Fisher information then becomes\\n(Jij)t:T =\\n\\n1JD K.p (F, r, D ) \\\\/\\n\\nflt:l\\n0\\\"1) ~\\n0\\\";\\nVij,\\n17\\n\\n(6)\\n\\nwhere K.p depends on the geometry of the tuning curves and is defined in [13].\\n\\n3 Results\\nIn this section, we consider different distributions of tuning widths in (6) and discuss advantageous and disadvantageous strategies for obtaining a high representational accuracy\\nin the neural population.\\nRadially symmetric tuning curves.\\nthe tuning-width distribution reads\\n\\nFor radially symmetric tuning curves of width a,\\nD\\n\\nPt:T(O\\\"l, .. . ,O\\\"D)\\n\\n= II O(O\\\"i -a);\\ni=l\\n\\nsee Fig. 1a for a schematic visualization of the arrangement of the tuning widths for the\\ncase D = 2. The average population Fisher information (6) for i = j becomes\\n(Jii)t:T =\\n\\n1JDK.p(F, r, D) aD -\\n\\n2,\\n\\n(7)\\n\\na result already obtained by Zhang and Sejnowski [13]. Equation (7) basically shows that\\nthe minimal estimation error increases with a for D = 1, that it does not depend on a for\\nD = 2, and that it decreases as a increases for D 2: 3. We shall discuss the relevance of\\nthis case below.\\nIdentical tuning curves without radial symmetry. Next we discuss tuning curves which\\nare identical but not radially symmetric; the tuning-width distribution for this case is\\nD\\n\\nPt:T(0\\\"1, . .. ,O\\\"D)\\n\\n=\\n\\nII\\n\\nO(O\\\"i -\\n\\nad,\\n\\ni=l\\n\\nwhere ai denotes the fixed width in dimension i. For i = j, the average population Fisher\\ninformation (6) reduces to [11,4]\\n(Jii)t:T = 1JDK.p ( F,\\n\\nr,\\n\\nD)\\n\\nDfl 1=1\\n0\\\"1\\n-2\\n\\nO\\\"i\\n\\n.\\n\\n(8)\\n\\n\\fc.\\n\\n118\\n\\n(a)\\n\\nW. Eurich, S. D. Wilke and H. Schwegler\\n\\n(b)\\n\\n/\\n\\nFigure 1: Visualization of different distributions of\\ntuning widths for D = 2. (a) Radially symmetric tuning curves. The dot indicates a fixed (j, while the diagonalline symbolizes a variation in (j discussed in [13].\\n(b) Identical tuning curves which are not radially symmetric. (c) Tuning widths uniformly distributed within\\na small rectangle. (d) Two sUbpopulations each of\\nwhich is narrowly tuned in one dimension and broadly\\ntuned in the other direction.\\n\\n.\\n\\n(c)\\n\\n(d)\\n\\n.\\n\\nb _ b\\n2\\n\\n.\\n\\nEquation (8) contains (7) as a special case. From (8) it becomes immediately clear that the\\nexpected minimal square encoding error for the i-th stimulus feature, ?~ min = 1/ (Jii(X))u,\\ndepends on i, i. e., the population specializes in certain features. The error obtained in\\ndimension i thereby depends on the tuning widths in all dimensions.\\nWhich encoding strategy is optimal for a population whose task it is to encode a single\\nfeature, say feature i, with high accuracy while not caring about the other dimensions? In\\norder to answer this question, we re-write (8) in terms of receptive field overlap.\\nFor the tuning functions f(k) (x) encountered empirically, large values ofthe single-neuron\\nFisher information (4) are typically restricted to a region around the center of the tuning\\nfunction, c(k). The fraction p({3) of the Fisher information that falls into a region ED\\nJ~(k)2 ~ (3 aroundc(k) is given by\\n\\nf\\np({3)\\n\\n:=\\n\\nd\\n\\nE; d\\nX\\n\\nD\\n\\nD\\n\\n\\\"\\\",D\\nX L....i=l\\n\\nX\\n\\n(k) ( )\\nJ ii X\\n\\n2:~t=l J~~)\\n( )\\nu\\nX\\n\\nj3\\n\\nf\\n\\nd~ ~D+l At/>(e, F, T)\\n\\no\\n\\n(9)\\n\\n00\\n\\nf\\n\\nd~ ~D+l At/>(~2, F, T)\\n\\no\\n\\nwhere the index (k) was dropped because the tuning curves are assumed to have identical shapes. Equation (9) allows the definition of an effective receptive field, RF~~,\\ninside of which neuron k conveys a major fraction Po of Fisher information, RF~~ :=\\n\\n{xl~ ~ {3o} , where (3o is chosen such that p({3o)\\n\\n= Po. The Fisher information a\\n\\nneuron k carries is small unless x E RF~~. This has the consequence that a fixed stimulus\\nx is actually encoded only by a subpopulation of neurons. The point x in stimulus space is\\ncovered by\\n27r D/ 2({30)D D _\\n(10)\\nNcode:= 1] Dr(D/2)\\n(Jj\\n\\n}1\\n\\nreceptive fields. With the help of (10), the average population Fisher information (8) can\\nbe re-written as\\n(11)\\n\\nEquation (11) can be interpreted as follows: We assume that the population of neurons\\nencodes stimulus dimension i accurately, while all other dimensions are of secondary importance. The average population Fisher information for dimension i, (Jii ) u, is determined\\nby the tuning width in dimension i, (ji, and by the size of the active subpopulation, N code '\\nThere is a tradeoff between these quantities. On the one hand, the encoding error can be\\ndecreased by decreasing (ji, which enhances the Fisher information carried by each single\\n\\n\\fNeural Representation ofMulti-Dimensional Stimuli\\n\\n119\\n\\nneuron. Decreasing ai, on the other hand, will also shrink the active subpopulation via\\n(10). This impairs the encoding accuracy, because the stimulus position is evaluated from\\nthe activity of fewer neurons. If (11) is valid due to a sufficient receptive field overlap,\\nNcode can be increased by increasing the tuning widths, aj, in all other dimensions j i- i.\\nThis effect is illustrated in Fig. 2 for D = 2.\\n\\nX2\\nc=:>\\n\\nx2, s\\n\\nX2\\n,II\\\"\\\\..\\\\\\n\\nU\\nx2,s\\n\\nFigure 2: Encoding strategy for a stimulus characterized by parameters Xl,s and X2,s' Feature Xl is to be encoded accurately. Effective receptive field shapes are indicated for both\\npopulations. If neurons are narrowly tuned in X2 (left), the active population (solid) is\\nsmall (here: Ncode = 3). Broadly tuned receptive fields for X2 (right) yield a much larger\\npopulation (here: Ncode = 27) thus increasing the encoding accuracy.\\nIt shall be noted that although a narrow tuning width ai is advantageous, the limit ai ---t 0\\nyields a bad representation. For narrowly tuned cells, gaps appear between the receptive\\nfields: The condition 17(X) == const. breaks down, and (6) is no longer valid. A more\\ndetailed calculation shows that the encoding error diverges as ai --* 0 [4]. The fact that\\nthe encoding error decreases for both narrow tuning and broad tuning - due to (11) - proves\\nthe existence of an optimal tuning width, An example is given in Fig. 3a.\\n3\\n\\nrTI~--~------~----~------~\\n\\n1\\\\\\n\\n(b)\\n\\nIi\\n\\n1\\\\\\n\\nIi\\n\\n0.8\\n\\nII\\nII\\nI;\\n\\n2\\n\\n1\\\\\\n\\nI ,\\n\\n;to.6\\n~\\n\\n~~~~;::~-:.~~;:\\n\\nA\\n\\nN~O.4\\nw\\n\\n----- ---- ----- -- ---\\n\\nv\\n\\n0.2\\n\\nO'----~--~--~-----'-------'\\n\\no\\n\\n0.5\\n\\n1\\nA\\n\\n1.5\\n\\n2\\n\\nFigure 3: (a) Example for the encoding behavior with narrow tuning curves arranged on\\na regular lattice of dimension D = 1 (grid spacing ~). Tuning curves are Gaussian, and\\nneural firing is modeled as a Poisson process, Dots indicate the minimal square encoding\\nerror averaged over a uniform distribution of stimuli, (E~in)' as a function ofa. The minimum is clearly visible. The dotted line shows the corresponding approximation according\\nto (8). The inset shows Gaussian tuning curves of optimal width, ao pt ~ 0.4~. (b) 9D()..)\\nas a function of ).. for different values of D.\\n\\n\\fc. W.\\n\\n120\\n\\nEurich, S. D. Wilke and H. Schwegler\\n\\nNarrow distribution of tuning curves. In order to study the effects of encoding the\\nstimulus with distributed tuning widths instead of identical tuning widths as in the previous\\ncases, we now consider the distribution\\n\\ng:i e\\nD\\n\\nPu(lT1,'\\\" ,lTD)\\n\\n=\\n\\n[lTi - (O'i -\\n\\ni)] e [(O'i + i) -lTi] ,\\n\\n(12)\\n\\ne\\n\\ndenotes the Heaviside step function. Equation (12) describes a uniform distriwhere\\nbution in a D-dimensional cuboid of size b1 , ... , bD around (0'1, .. . 0'D); cf. Fig. 1c. A\\nstraightforward calculation shows that in this case, the average population Fisher information (6) for i = j becomes\\n\\n(Jii)u\\n\\n= f/DKtj) (F, T, D) n~l\\nO'~ 0'1\\n\\n{\\n\\n1\\n1 + 12\\n\\n(bO'i 2+ 0 [( O'ib 4] }.\\ni )\\n\\ni )\\n\\n(13)\\n\\nA comparison with (8) yields the astonishing result that an increase in bi results in an\\nincrease in the i-th diagonal element of the average population Fisher information matrix\\nand thus in an improvement in the encoding of the i-th stimulus feature, while the encoding\\nin dimensions j :f. i is not affected. Correspondingly, the total encoding error can be\\ndecreased by increasing an arbitrary number of edge lengths of the cube. The encoding by\\na population with a variability in the tuning curve geometries as described is more precise\\nthan that by a uniform population. This is true/or arbitrary D. Zhang and Sejnowski [13]\\nconsider the more artificial situation of a correlated variability ofthe tuning widths: tuning\\ncurves are always assumed to be radially symmetric. This is indicated by the diagonal\\nline in Fig. 1a. A distribution of tuning widths restricted to this subset yields an average\\npopulation Fisher information ex: (O'D-2) and does not improve the encoding for D = 2 or\\n\\nD=3.\\nFragmentation into D subpopulations. Finally, we study a family of distributions of\\ntuning widths which also yields a lower minimal encoding error than the uniform population. Let the density of tuning curves be given by\\n1 D\\n\\nPu(lT1,'\\\" ,lTD) = D\\n\\nL 6( lTi i=l\\n\\nAO')\\n\\nII 6(lTj - 0'),\\n\\n(14)\\n\\nj?-i\\n\\nwhere A > O. For A = 1, the population is uniform as in (7). For A :f. 1, the population\\nis split up into D subpopulations; in subpopulation i, lTi is modified while lTj == 0' for\\nj :f. i. See Fig. Id for an example. The diagonal elements ofthe average population Fisher\\ninformation are\\n\\n(Jii)u\\n\\n{1 + (D = f/DKtj)(F, T, D) -D-2\\nIT\\nDA\\n\\nI)A 2 }\\n\\n'\\n\\n(15)\\n\\nwhere the term in brackets will be abbreviated as 9D(A). (Jii)u does not depend on i in\\nthis case because of the symmetry in the sUbpopulations. Equation (15) and the uniform\\ncase (7) differ by 9D(A) which will now be discussed. Figure 3b shows 9D(A) for different\\nvalues of D. For A = 1, 9D(A) = 1 and (7) is recovered as expected. 9D(A) = 1\\nalso holds for A = 1/ (D - 1) < 1: narrowing one tuning width in each subpopulation\\nwill at first decrease the resolution provided D 2: 3; this is due to the fact that Ncode is\\ndecreased. For A < 1/(D - 1), however, 9D(A) > 1, and the resolution exceeds (Jii)u in\\n(7) because each neuron in the i-th subpopulation carries a high Fisher information in the\\ni-th dimension. D = 2 is a special case where no impairment of encoding occurs because\\nthe effect of a decrease of Ncode is less pronounced. Interestingly, an increase in A also\\nyields an improvement in the encoding accuracy. This is a combined effect resulting from\\nan increase in Ncode on the one hand and the existence of D subpopulations, D - 1 of\\n\\n\\fNeural Representation of Multi-Dimensional Stimuli\\n\\n121\\n\\nwhich maintain their tuning widths in each dimension on the other hand. The discussion\\nof 9D(>\\\") leads to the following encoding strategy. For small >.., (Jii)u increases rapidly,\\nwhich suggests a fragmentation of the population into D subpopulations each of which\\nencodes one feature with high accuracy, i.e., one tuning width in each subpopulation is\\nsmall whereas the remaining tuning widths are broad. Like in the case discussed above, the\\ntheoretical limit of this method is a breakdown of the approximation of TJ == const. and the\\nvalidity of (6) due to insufficient receptive field overlap.\\n\\n4 Discussion and Outlook\\nWe have discussed the effects of a variation of the tuning widths on the encoding accuracy\\nobtained by a population of stochastically spiking neurons. The question of an optimal\\ntuning strategy has turned out to be more complicated than previously assumed. More\\nspecifically, the case which focused most attention in the literature - radially symmetric\\nreceptive fields [5, 1,9, 3, 13] - yields a worse encoding accuracy than most other cases we\\nhave studied: uniform populations with tuning curves which are not radially symmetric;\\ndistributions of tuning curves around some symmetric or non-symmetric tuning curve; and\\nthe fragmentation of the population into D subpopulations each of which is specialized in\\none stimulus feature.\\nIn a next step, the theoretical results will be compared to empirical data on encoding properties of neural popUlations. One aspect is the existence of sensory maps which consist\\nof neural subpopulations with characteristic tuning properties for the features which are\\nrepresented. For example, receptive fields of auditory neurons in the midbrain of the barn\\nowl have elongated shapes [6]. A second aspect concerns the short-term dynamics of receptive fields. Using single-unit recordings in anaesthetized cats, Worgotter et al. [12]\\nobserved changes in receptive field size taking place in 50-lOOms. Our findings suggest\\nthat these dynamics alter the resolution obtained for the corresponding stimulus features.\\nThe observed effect may therefore realize a mechanism of an adaptable selective signal\\nprocessing.\\n\\nReferences\\n[1] Baldi, P. & HeiJigenberg, W. (1988) BioI. Cybern. 59:313-318.\\n[2] Deco, G. & Obradovic, D. (1997) An Information-Theoretic Approach to Neural Computing.\\nNew York: Springer.\\n[3] Eurich, C. W. & Schwegler, H. (1997) BioI. Cybern. 76: 357-363.\\n\\n[4] Eurich, C. W. & Wilke, S. D. (2000) NeuraL Compo (in press).\\n[5] Hinton, G. E., McClelland, J. L. & Rumelhart, D. E (1986) In Rumelhart, D. E. & McClelland,\\nJ. L. (eds.), ParaLLeL Distributed Processing, Vol. 1, pp. 77-109. Cambridge MA: MIT Press.\\n[6] Knudsen, E. I. & Konishi, M. (1978) Science 200:795-797.\\n[7] Kuffter, S. W. (1953) 1. Neurophysiol. 16:37-68.\\n[8] Lettvin, J. Y., Maturana, H. R., McCulloch, W. S. & Pitts, W. H. (1959) Proc. Inst. Radio Eng.\\nNY 47:1940-1951.\\n[9] Snippe, H. P. & Koenderink, J. J. (1992) BioI. Cybern. 66:543-551.\\n[10] Wiggers, W., Roth, G., Eurich, C. W. & Straub, A. (1995) J. Camp. Physiol. A 176:365-377.\\n[11] Wilke, S. D. & Eurich, C. W. (1999) In Verleysen, M. (ed.), ESANN 99, European Symposium\\non Artificial Neural Networks, pp. 435-440. Brussels: D-Facto.\\n[12] Worgotter, F., Suder, K., Zhao, Y., Kerscher, N., Eysel, U. T. & Funke, K. (1998) Nature\\n396:165-168.\\n[13] Zhang, K. & Sejnowski, T. J. (1999) NeuraL Compo 11:75-84.\\n\\n\\f\",\n          \"Searching for Character Models\\n\\nJaety Edwards\\nDepartment of Computer Science\\nUC Berkeley\\nBerkeley, CA 94720\\njaety@cs.berkeley.edu\\n\\nDavid Forsyth\\nDepartment of Computer Science\\nUC Berkeley\\nBerkeley, CA 94720\\ndaf@cs.berkeley.edu\\n\\nAbstract\\nWe introduce a method to automatically improve character models for a\\nhandwritten script without the use of transcriptions and using a minimum\\nof document specific training data. We show that we can use searches for\\nthe words in a dictionary to identify portions of the document whose\\ntranscriptions are unambiguous. Using templates extracted from those\\nregions, we retrain our character prediction model to drastically improve\\nour search retrieval performance for words in the document.\\n\\n1 Introduction\\nAn active area of research in machine transcription of handwritten documents is reducing\\nthe amount and expense of supervised data required to train prediction models. Traditional\\nOCR techniques require a large sample of hand segmented letter glyphs for training. This\\nper character segmentation is expensive and often impractical to acquire, particularly if the\\ncorpora in question contain documents in many different scripts.\\nNumerous authors have presented methods for reducing the expense of training data by\\nremoving the need to segment individual characters. Both Kopec et al [3] and LeCun et al\\n[5] have presented models that take as input images of lines of text with their ASCII transcriptions. Training with these datasets is made possible by explicitly modelling possible\\nsegmentations in addition to having a model for character templates.\\nIn their research on ?wordspotting?, Lavrenko et al [4] demonstrate that images of entire\\nwords can be highly discriminative, even when the individual characters composing the\\nword are locally ambiguous. This implies that images of many sufficiently long words\\nshould have unambiguous transcriptions, even when the character models are poorly tuned.\\nIn our previous work, [2], the discriminatory power of whole words allowed us to achieve\\nstrong search results with a model trained on a single example per character.\\nThe above results have shown that A) one can learn new template models given images of\\ntext lines and their associated transcriptions, [3, 5] without needing an explicit segmentation\\nand that B) entire words can often be identified unambiguously, even when the models for\\nindividual characters are poorly tuned. [2, 4]. The first of these two points implies that\\ngiven a transcription, we can learn new character models. The second implies that for at\\nleast some parts of a document, we should be able to provide that transcription ?for free?,\\nby matching against a dictionary of known words.\\n\\n\\fs1\\n\\ns2\\n\\ns3\\n\\ns4\\n\\ns5\\n\\ns6\\n\\ns7\\n\\ns8\\n\\n?d\\n\\ndi\\n\\nix\\n\\nxe\\n\\ner\\n\\nri\\n\\nis\\n\\ns?\\n\\nFigure 1: A line, and the states that generate it. Each state st is defined by its left and\\nright characters ctl and ctr (eg ?x? and ?e? for s4 ). In the image, a state spans half of each\\nof these two characters, starting just past the center of the left character and extending to\\nthe center of the right character, i.e. the right half of the ?x? and the left half of the ?e?\\nin s4 . The relative positions of the two characters is given by a displacement vector dt\\n(superimposed on the image as white lines). Associating states with intracharacter spaces\\ninstead of with individual characters allows for the bounding boxes of characters to overlap\\nwhile maintaining the independence properties of the Markov chain.\\nIn this work we combine these two observations in order to improve character models\\nwithout the need for a document specific transcription. We provide a generic dictionary of\\nwords in the target language. We then identify ?high confidence? regions of a document.\\nThese are image regions for which exactly one word from our dictionary scores highly\\nunder our model. Given a set of high confidence regions, we effectively have a training\\ncorpus of text images with associated transcriptions. In these regions, we infer a segmentation and extract new character examples. Finally, we use these new exemplars to learn\\nan improved character prediction model. As in [2], our document in this work is a 12th\\ncentury manuscript of Terence?s Comedies obtained from Oxford?s Bodleian library [1].\\n\\n2 The Model\\nHidden Markov Models are a natural and widely used method for modeling images of text.\\nIn their simplest incarnation, a hidden state represents a character and the evidence variable\\nis some feature vector calculated at points along the line. If all characters were known to\\nbe of a single fixed width, this model would suffice. The probability of a line under this\\nmodel is given as\\nY\\np(line) = p(c1 |?)\\np(ct |ct?1 )p(im[w?(t?1):w?t]|ct )\\n(1)\\nt>1\\n\\nwhere ct represents the tth character on the line, ? represents the start state, w is the width\\nof a character, and im[w(t?1)+1:wt] represents the column of pixels beginning at column\\nw ? (t ? 1) + 1 of the image and ending at column w ? t, (i.e. the set of pixels spanned by\\nc)\\nUnfortunately, character?s widths do vary quite substantially and so we must extend the\\nmodel to accommodate different possible segmentations. A generalized HMM allows us to\\ndo this. In this model a hidden state is allowed to emit a variable length series of evidence\\nvariables. We introduce an explicit distribution over the possible widths of a character.\\nLetting dt be the displacement vector associated with the tth character, and ctx refer to the\\nx location of the left edge of a character on the line, the probability of a line under this\\nrevised model is\\nY\\np(line) = p(c1 |?)\\np(ct |ct?1 )p(dt |ct )p(im[ctx +1:ctx +d] |dt , ct )\\n(2)\\nt>1\\n\\nThis is the model we used in [2]. It performs far better than using an assumption of fixed\\nwidths, but it still imposes unrealistic constraints on the relative positions of characters. In\\n\\n\\fparticular, the portion of the ink generated by the current character is assumed to be independent of the preceding character. In other words, the model assumes that the bounding\\nboxes of characters do not overlap. This constraint is obviously unrealistic. Characters\\nroutinely overlap in our documents. ?f?s, for instance, form ligatures with most following characters. In previous work, we treated this overlap as noise, hurting our ability to\\ncorrectly localize templates. Under this model, local errors of alignment would also often propagate globally, adversely affecting the segmentation of the whole line. For search,\\nthis noisy segmentation still provides acceptable results. In this work, however, we need\\nto extract new templates, and thus correct localization and segmentation of templates is\\ncrucial.\\nIn our current work, we have relaxed this constraint, allowing characters to partially overlap. We achieve this by changing hidden states to represent character bigrams instead of\\nsingle characters (Figure 1). In the image, a state now spans the pixels from just past the\\ncenter of the left character to the pixel containing the center of the right character. We\\nadjust our notation somewhat to reflect this change, letting st now represent the tth hidden state and ctl and ctr be the left and right characters associated with s. dt is now the\\ndisplacement vector between the centers of ctl and ctr .\\nThe probability of a line under this, our actual, model is\\nY\\np(line) = p(s1 |?)\\np(st |st?1 )p(dt |ctl , ctr )p(im[stx +1:stx +dt ]|ctl , ctr , dt )\\n\\n(3)\\n\\nt>1\\n\\nThis model allows overlap of bounding boxes, but it does still make the assumption that\\nthe bounding box of the current character does not extend past the center of the previous\\ncharacter. This assumption does not fully reflect reality either. In Figure 1, for example,\\nthe left descender of the x extends back further than the center of the preceding character.\\nIt does, however, accurately reflect the constraints within the heart of the line (excluding\\nascenders and descenders). In practice, it has proven to generate very accurate segmentations. Moreover, the errors we do encounter no longer tend to affect the entire line, since\\nthe model has more flexibility with which to readjust back to the correct segmentation.\\n2.1 Model Parameters\\nOur transition distribution between states is simply a 3-gram character model. We train this\\nmodel using a collection of ASCII Latin documents collected from the web. This set does\\nnot include the transcriptions of our documents.\\nConditioned on displacement vector, the emission model for generating an image chunk\\ngiven a state is a mixture of gaussians. We associate with each character a set of image\\nwindows extracted from various locations in the document. We initialize these sets with\\none example a piece from our hand cut set (Figure 2). We adjust the probability of an image\\ngiven the state to include the distribution over blocks by expanding the last term of Equation\\n3 to reflect this mixture. Letting bck represent the k th exemplar in the set associated with\\ncharacter c, the conditional probability of an image region spanning the columns from x to\\nx? is given as\\nX\\np(imx:x? |ctl , ctr , dt ) =\\np(imx:x? |bctl i , bctr j , dt )\\n(4)\\ni,j\\n\\nIn principle, the displacement vectors should now be associated with an individual block,\\nnot a character. This is especially true when we have both upper and lower case letters.\\nHowever, our model does not seem particularly sensitive to this displacement distribution\\nand so in practice, we have a single, fairly loose, displacement distribution per character.\\nGiven a displacement vector, we can generate the maximum likelihood template image\\nunder our model by compositing the correct halves of the left and right blocks. Reshaping\\n\\n\\fthe image window into a vector, the likelihood of an image window is then modeled as\\na gaussian, using the corresponding pixels in the template as the means, and assuming\\na diagonal covariance matrix. The covariance matrix largely serves to mask out empty\\nregions of a character?s bounding box, so that we do not pay a penalty when the overlap of\\ntwo characters? bounding boxes contains only whitespace.\\n2.2 Efficiency Considerations\\nThe number of possible different templates for a state is O(|B| ? |B| ? |D|), where |B| is\\nthe number of different possible blocks and |D| is the number of candidate displacement\\nvectors. To make inference in this model computationally feasible, we first restrict the\\ndomain of d. For a given pair of blocks bl and br , we consider only displacement vectors\\nwithin some small x distance from a mean displacement mbl ,br , and we have a uniform\\ndistribution within this region. m is initialized from the known size of our single hand cut\\ntemplate. In the current work, we do not relearn the m. These are held fixed and assumed\\nto be the same for all blocks associated with the same letter.\\nEven when restricting the number of d?s under consideration as discussed above, it is computationally infeasible to consider every possible location and pair of blocks. We therefore\\nprune our candidate locations by looking at the likelihood of blocks in isolation and only\\nconsidering locations where there is a local optimum in the response function and whose\\nvalue is better than a given threshold. In this case our threshold for a given location is that\\nL(block) < .7L(background) (where L(x) represents the negative log likelihood of x).\\nIn other words, a location has to look at least marginally more like a given block than it\\nlooks like the background.\\nAfter pruning locations in this manner, we are left with a discrete set of ?sites,? where we\\ndefine a site as the tuple (block type, x location, y location). We can enumerate the set of\\npossible states by looking at every pair of sites whose displacement vector has a non-zero\\nprobability.\\n2.3 Inference In The Model\\nThe statespace defined above is a directed acyclic graph, anchored at the left edge and\\nright edges of a line of text. A path through this lattice defines both a transcription and\\na segmentation of the line into individual characters. Inference in this model is relatively\\nstraightforward because of our constraint that each character may overlap only one preceding and one following character, and our restriction of displacement vectors to a small\\ndiscrete range. The first restriction means that we need only consider binary relations between templates. The second preserves the independence relationships of an HMM. A\\ngiven state st is independent of the rest of the line given the values of all other states within\\ndmax of either edge of st (where dmax is the legal displacement vector with the longest\\nx component.) We can therefore easily calculate the best path or explicitly calculate the\\nposterior of a node by traversing the state graph in topological order, sorted from left to\\nright. The literature on Weighted Finite State Transducers ([6], [5]) is a good resource for\\nefficient algorithms on these types of statespace graph.\\n\\n3 Learning Better Character Templates\\nWe initialize our algorithm with a set of handcut templates, exactly 1 per character, (Figure\\n2), and our goal is to construct more accurate character models automatically from unsupervised data. As noted above, we can easily calculate the posterior of a given site under\\nour model. (Recall that a site is a particular character template at a given (x,y) location in\\nthe line.) The traditional EM approach to estimating new templates would be to use these\\n\\n\\fFigure 2: Original Training Data These 22 glyphs are our only document specific training\\ndata. We use the model based on these characters to extract the new examples shown below\\n\\nFigure 3: Examples of extracted templates We extract new templates from high confidence\\nregions. From these, we choose a subset to incorporate into the model as new exemplars.\\nTemplates are chosen iteratively to best cover the space of training examples. Notice that\\nfor ?q? and ?a?, we have extracted capital letters, of which there were no examples in\\nour original set of glyphs. This happens when the combination of constraints from the\\ndictionary the surrounding glyphs make a ?q? or ?a? the only possible explanation for\\nthis region, even though its local likelihood is poor.\\n\\nsites as training examples, weighted by their posteriors. Unfortunately, the constraints imposed by 3 and even 4-gram character models seem to be insufficient. The posteriors of\\nsites are not discriminative enough to get learning off the ground.\\nThe key to successfully learning new templates lies is the observation from our previous\\nwork [2], that even when the posteriors of individual characters are not discriminative, one\\ncan still achieve very good search results with the same model. The search word in effect\\nserves as its own language model, only allowing paths through the state graph that actually\\ncontain it, and the longer the word the more it constrains the model. Whole words impose\\nmuch tighter constraints than a 2 or 3-gram character model, and it is only with this added\\npower that we can successfully learn new character templates.\\nWe define the score for a search as the negative log likelihood of the best path containing\\nthat word. With sufficiently long words, it becomes increasingly unlikely that a spurious\\npath will achieve a high score. Moreover, if we are given a large dictionary of words and\\nno alternative word explains a region of ink nearly as well as the best scoring word, then\\nwe can be extremely confident that this is a true transcription of that piece of ink.\\nStarting with a weak character model, we do not expect to find many of these ?high confidence? regions, but with a large enough document, we should expect to find some. From\\nthese regions, we can extract new, reliable templates with which to improve our character\\nmodels. The most valuable of these new templates will be those that are significantly different from any in our current set. For example, in Figure 3, note that our system identifies\\ncapital Q?s, even though our only input template was lower case. It identifies this ink as\\na Q in much the same way that a person solves a crossword puzzle. We can easily infer\\nthe missing character in the string ?obv-ous? because the other letters constrain us to one\\npossible solution. Similarly, if other character templates in a word match well, then we can\\nunambiguously identify the other, more ambiguous ones. In our Latin case, ?Quid? is the\\nonly likely explanation for ?-uid?.\\n3.1 Extracting New Templates and Updating The Model\\nWithin a high confidence region we have both a transcription and a localization of template\\ncenters. It remains only to cut out new templates. We accomplish this by creating a template\\nimage for the column of pixels from the corresponding block templates and then assigning\\nimage pixels to the nearest template character (measured by Euclidean distance).\\nGiven a set of templates extracted from high confidence regions, we choose a subset of\\n\\n\\fScore Under Model\\n\\nworse\\n3400\\n3350\\n3300\\nbest\\nConfidence Margins\\n\\nFigure 4: Each line segment in the lower figure represents a proposed location for a word\\nfrom our dictionary. It?s vertical height is the score of that location under our model. A\\nlower score represents a better fit. The dotted line is the score of our model?s best possible\\npath. Three correct words, ?nec?, ?quin? and ?dari?, are actually on the best path. We\\ndefine the confidence margin of a location as the difference in score between the best\\nfitting word from our dictionary and the next best.\\n\\nFigure 5: Extracting Templates For a region with sufficiently high confidence margin, we\\nconstruct the maximum likelihood template from our current exemplars. left, and we assign\\npixels from the original image to a template based on its distance to the nearest pixel in\\nthe template image, extracting new glyph exemplars right. These new glyphs become the\\nexemplars for our next round of training.\\n\\ntemplates that best explain the remaining examples. We do this in a greedy fashion by\\nchoosing the example whose likelihood is lowest under our current model and adding it to\\nour set. Currently, we threshold the number of new templates for the sake of efficiency. Finally, given the new set of templates, we can add them to the model and rerun our searches,\\npotentially identifying new high confidence regions.\\n\\n4 Results\\nOur algorithm iteratively improves the character model by gathering new training data from\\nhigh confidence regions. Figure 3 shows that this method finds new templates significantly\\ndifferent from the originals. In this document, our set of examples after one round appears\\nto cover the space of character images well, at least those in lower case. Our templates are\\nnot perfect. The ?a?, for instance, has become associated with at least one block that is in\\nfact an ?o?. These mistakes are uncommon, particularly if we restrict ourselves to longer\\nwords. Those that do occur introduce a tolerable level noise into our model. They make\\ncertain regions of the document more ambiguous locally, but that local ambiguity can be\\novercome with the context provided by surrounding characters and a language model.\\nImproved Character Models We evaluate the method more quantitatively by testing the\\nimpact of the new templates on the quality of searches performed against the document.\\nTo search for a given word, we rank lines by the ratio of the maximum likelihood transcription/segmentation that contains the search word to the likelihood of the best possible\\nsegmentation/transcription under our model. The lowest possible search score is 1, happening when the search word is actually a substring of the maximum likelihood transcription.\\nHigher scores mean that the word is increasingly unlikely under our model. In Figure 7, the\\nfigure on the left shows the improvement in ranking of the lines that truly contain selected\\nsearch words. The odd rows (in red) are search results using only the original 22 glyphs,\\n\\n\\f20\\n40\\n60\\n80\\n100\\n\\n200\\n\\n300\\n\\n400\\n\\n500\\n\\n600\\n\\nRnd 2\\n\\nRnd 1\\n\\n2700\\n2650\\n2600\\ndotted (wrong):\\nsolid (correct):\\n1920\\n1900\\n1880\\n1860\\n1840\\ndotted (wrong):\\nsolid (correct):\\n\\niam\\n\\nnupta\\nnuptiis\\n\\ninquam\\n\\n(v|u)ideo\\nvidet\\n\\nnupta\\nnuptiis\\n\\npost inquam\\npostquam\\n\\n(v|u)ideo\\nvidet\\n\\nFigure 6: Search Results with (Rnd 1) initial templates only and with (Rnd 2) templates\\nextracted from high confidence regions. We show results that have a score within 5% of the\\nbest path. Solid Lines are the results for the correct word. Dotted lines represent other\\nsearch results, where we have made a few larger in order to show those words that are\\nthe closest competitors to the true word. Many alternative searches, like the highlighted\\n?post? are actually portions of the correct larger words. These restrict our selection of\\nconfidence regions, but do not impinge on search quality.\\nEach correct word has significantly improved after one round of template reestimation.\\n?iam? has been correctly identified, and is a new high confidence region. Both ?nuptiis?\\nand ?postquam? are now the highest likelihood words for their region barring smaller\\nsubsequences, and ?videt? has narrowed the gap between its competitor ?video?.\\nwhile the even rows (in green) use an additional 332 glyphs extracted from high confidence\\nregions. Search results are markedly improved in the second model. The word ?est?, for\\ninstance, only had 15 of 24 of the correct lines in the top 100 under the original model,\\nwhile under the learned model all 24 are not only present but also more highly ranked.\\nImproved Search Figure 6 shows the improved performance of our refitted model for\\na single line. Most words have greatly improved relative to their next best alternative.\\n?postquam? and ?iam? were not even considered by the original model and now are nearly\\noptimal. The right of Figure 7 shows the average precision/recall curve under each model\\nfor 21 words with more than 4 occurrences in the dataset. Precision is the percentage\\nof lines truly containing a word in the top n search results, and recall is the percentage\\nof all lines containing the word returned in the top n results. The learned model clearly\\ndominates. The new model also greatly improves performance for rare words. For 320\\nwords ocurring just once in the dataset, 50% are correctly returned as the top ranked result\\nunder the original model. Under the learned model, this number jumps to 78%.\\n\\n5 Conclusions and Future Work\\nIn most fonts, characters are quite ambiguous locally. An ?n? looks like a ?u?, looks like\\n?ii?, etc. This ambiguity is the major hurdle to the unsupervised learning of character\\ntemplates. Language models help, but the standard n-gram models provide insufficient\\nconstraints, giving posteriors for character sites too uninformative to get EM off the ground.\\n\\n\\fAggregate Precision/Recall Curve\\n\\nSelected Words, Top 100 Returned Lines\\n\\nPrecision\\n\\nest\\n(15,24)/24\\nnescio\\n( 1, 1)/ 1\\npostquam\\n( 0, 2)/ 2\\nquod\\n(14,14)/14\\nmoram\\n( 0, 2)/ 2\\nnon\\n( 8, 8)/ 8\\nquid\\n( 9, 9)/ 9\\n10 20 30 40 50 60 70 80 90100\\n\\n0.75\\n0.7\\n0.65\\n0.6\\n0.55\\n0.5\\n0.45\\n0.4\\n0.35\\n\\nOriginal Model\\nRefit Model\\n0.2\\n\\n0.4\\n0.6\\nRecall\\n\\n0.8\\n\\n1\\n\\nFigure 7: The figure on the left shows the those lines with the top 100 scores that actually\\ncontain the specified word. The first of each set of two rows (in red) is the results from\\nRound 1. The second (in green) is the results for Round 2. Almost all search words in our\\ncorpus show a significant improvement. The numbers to the right (x/y) mean that out of\\ny lines that actually contained the search word in our document, x of them made it into\\nthe top ten. On the right are average precision/recall curves for 21 high frequency words\\nunder the model with our original templates (Rnd 1) and after refitting with new extracted\\ntemplates (Rnd 2). Extracting new templates vastly improves our search quality\\nAn entire word is much different. Given a dictionary, we expect many word images to have\\na single likely transcription even if many characters are locally ambiguous. We show that\\nwe can identify these high confidence regions even with a poorly tuned character model. By\\nextracting new templates only from these regions of the document, we overcome the noise\\nproblem and significantly improve our character models. We demonstrate this improvement\\nfor the task of search where the refitted models have drastically better search responses than\\nwith the original. Our method is indifferent to the form of the actual character emission\\nmodel. There is a rich literature in character prediction from isolated image windows, and\\nwe expect that incorporating more powerful character models should provide even greater\\nreturns and help us in learning less regular scripts.\\nFinding high confidence regions to extract good training examples is a broadly applicable concept. We believe this work should extend to other problems, most notably speech\\nrecognition. Looked at more abstractly, our use of language model in this work is actually encoding spatial constraints. The probability of a character given an image window\\ndepends not only on the identify of surrounding characters but also on their spatial configuration. Integrating context into recognition problems is an area of intense research in\\nthe computer vision community, and we are investigating extending the idea of confidence\\nregions to more general object recognition problems.\\n\\nReferences\\n[1] Early Manuscripts at Oxford University. Bodleian library ms. auct. f. 2.13. http://image.ox.ac.uk/.\\n[2] J. Edwards, Y.W. Teh, D. Forsyth, R. Bock, M. Maire, and G. Vesom. Making latin manuscripts\\nsearchable using ghmm?s. In NIPS 17, pages 385?392. 2005.\\n[3] G. Kopec and M. Lomelin. Document-specific character template estimation. In Proceedings,\\nDocument Image Recognition III, SPIE, 1996.\\n[4] V. Lavrenko, T. Rath, and R. Manmatha. Holistic word recognition for handwritten historical\\ndocuments. In dial, pages 278?287, 2004.\\n[5] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document\\nrecognition. Proceedings of the IEEE, 86(11):2278?2324, 1998.\\n[6] M. Mohri, F. Pereira, and M. Riley. Weighted finite state transducers in speech recognition. ISCA\\nITRW Automatic Speech Recognition, pages 97?106, 2000.\\n\\n\\f\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "id": "HqofQEuFJLUY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d89af2b-a65b-4832-b040-4f1155b15611"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(7241, 7)"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.iloc[:5000, :]"
      ],
      "metadata": {
        "id": "4pIhiIDFJWDA"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "id": "mr_cdU7NJcpy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b527f185-fa60-417f-bd0e-429a4be03a68"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5000, 7)"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['paper_text'][0]"
      ],
      "metadata": {
        "id": "DRbBCiUWJeGF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 209
        },
        "outputId": "89555f65-56b7-4971-d9b7-07dbcfa206fc"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'767\\n\\nSELF-ORGANIZATION OF ASSOCIATIVE DATABASE\\nAND ITS APPLICATIONS\\nHisashi Suzuki and Suguru Arimoto\\nOsaka University, Toyonaka, Osaka 560, Japan\\nABSTRACT\\nAn efficient method of self-organizing associative databases is proposed together with\\napplications to robot eyesight systems. The proposed databases can associate any input\\nwith some output. In the first half part of discussion, an algorithm of self-organization is\\nproposed. From an aspect of hardware, it produces a new style of neural network. In the\\nlatter half part, an applicability to handwritten letter recognition and that to an autonomous\\nmobile robot system are demonstrated.\\n\\nINTRODUCTION\\nLet a mapping f : X -+ Y be given. Here, X is a finite or infinite set, and Y is another\\nfinite or infinite set. A learning machine observes any set of pairs (x, y) sampled randomly\\nfrom X x Y. (X x Y means the Cartesian product of X and Y.) And, it computes some\\nestimate j : X -+ Y of f to make small, the estimation error in some measure.\\nUsually we say that: the faster the decrease of estimation error with increase of the number of samples, the better the learning machine. However, such expression on performance\\nis incomplete. Since, it lacks consideration on the candidates of J of j assumed preliminarily. Then, how should we find out good learning machines? To clarify this conception,\\nlet us discuss for a while on some types of learning machines. And, let us advance the\\nunderstanding of the self-organization of associative database .\\n. Parameter Type\\nAn ordinary type of learning machine assumes an equation relating x\\'s and y\\'s with\\nparameters being indefinite, namely, a structure of f. It is equivalent to define implicitly a\\nset F of candidates of\\n(F is some subset of mappings from X to Y.) And, it computes\\nvalues of the parameters based on the observed samples. We call such type a parameter\\ntype.\\nFor a learning machine defined well, if F 3 f, j approaches f as the number of samples\\nincreases. In the alternative case, however, some estimation error remains eternally. Thus,\\na problem of designing a learning machine returns to find out a proper structure of f in this\\nsense.\\nOn the other hand, the assumed structure of f is demanded to be as compact as possible\\nto achieve a fast learning. In other words, the number of parameters should be small. Since,\\nif the parameters are few, some j can be uniquely determined even though the observed\\nsamples are few. However, this demand of being proper contradicts to that of being compact.\\nConsequently, in the parameter type, the better the compactness of the assumed structure\\nthat is proper, the better the learning machine. This is the most elementary conception\\nwhen we design learning machines .\\n\\n1.\\n\\n. Universality and Ordinary Neural Networks\\nNow suppose that a sufficient knowledge on f is given though J itself is unknown. In\\nthis case, it is comparatively easy to find out proper and compact structures of J. In the\\nalternative case, however, it is sometimes difficult. A possible solution is to give up the\\ncompactness and assume an almighty structure that can cover various 1\\'s. A combination\\nof some orthogonal bases of the infinite dimension is such a structure. Neural networks 1 ,2\\nare its approximations obtained by truncating finitely the dimension for implementation.\\n\\n? American Institute of Physics 1988\\n\\n\\x0c768\\nA main topic in designing neural networks is to establish such desirable structures of 1.\\nThis work includes developing practical procedures that compute values of coefficients from\\nthe observed samples. Such discussions are :flourishing since 1980 while many efficient methods have been proposed. Recently, even hardware units computing coefficients in parallel\\nfor speed-up are sold, e.g., ANZA, Mark III, Odyssey and E-1.\\nNevertheless, in neural networks, there always exists a danger of some error remaining\\neternally in estimating /. Precisely speaking, suppose that a combination of the bases of a\\nfinite number can define a structure of 1 essentially. In other words, suppose that F 3 /, or\\n1 is located near F. In such case, the estimation error is none or negligible. However, if 1\\nis distant from F, the estimation error never becomes negligible. Indeed, many researches\\nreport that the following situation appears when 1 is too complex. Once the estimation\\nerror converges to some value (> 0) as the number of samples increases, it decreases hardly\\neven though the dimension is heighten. This property sometimes is a considerable defect of\\nneural networks .\\n. Recursi ve Type\\nThe recursive type is founded on another methodology of learning that should be as\\nfollows. At the initial stage of no sample, the set Fa (instead of notation F) of candidates\\nof I equals to the set of all mappings from X to Y. After observing the first sample\\n(Xl, Yl) E X x Y, Fa is reduced to Fi so that I(xt) = Yl for any I E F. After observing\\nthe second sample (X2\\' Y2) E X x Y, Fl is further reduced to F2 so that i(xt) = Yl and\\nI(X2) = Y2 for any I E F. Thus, the candidate set F becomes gradually small as observation\\nof samples proceeds. The after observing i-samples, which we write\\nis one of the most\\nlikelihood estimation of 1 selected in fi;. Hence, contrarily to the parameter type, the\\nrecursive type guarantees surely that j approaches to 1 as the number of samples increases.\\nThe recursive type, if observes a sample (x\" yd, rewrites values 1,-l(X),S to I,(x)\\'s for\\nsome x\\'s correlated to the sample. Hence, this type has an architecture composed of a rule\\nfor rewriting and a free memory space. Such architecture forms naturally a kind of database\\nthat builds up management systems of data in a self-organizing way. However, this database\\ndiffers from ordinary ones in the following sense. It does not only record the samples already\\nobserved, but computes some estimation of l(x) for any x E X. We call such database an\\nassociative database.\\nThe first subject in constructing associative databases is how we establish the rule for\\nrewri ting. For this purpose, we adap t a measure called the dissimilari ty. Here, a dissimilari ty\\nmeans a mapping d : X x X -+ {reals > O} such that for any (x, x) E X x X, d(x, x) > 0\\nwhenever l(x) t /(x). However, it is not necessarily defined with a single formula. It is\\ndefinable with, for example, a collection of rules written in forms of \"if? .. then?? .. \"\\nThe dissimilarity d defines a structure of 1 locally in X x Y. Hence, even though\\nthe knowledge on f is imperfect, we can re:flect it on d in some heuristic way. Hence,\\ncontrarily to neural networks, it is possible to accelerate the speed of learning by establishing\\nd well. Especially, we can easily find out simple d\\'s for those l\\'s which process analogically\\ninformation like a human. (See the applications in this paper.) And, for such /\\'s, the\\nrecursive type shows strongly its effectiveness.\\nWe denote a sequence of observed samples by (Xl, Yd, (X2\\' Y2),???. One of the simplest\\nconstructions of associative databases after observing i-samples (i = 1,2,.,,) is as follows.\\n\\ni\\n\\ni\"\\n\\nI,\\n\\nAlgorithm 1. At the initial stage, let So be the empty set. For every i =\\n1,2\" .. , let i,-l(x) for any x E X equal some y* such that (x*,y*) E S,-l and\\n\\nd(x, x*) =\\n\\nmin\\n(%,y)ES.-t\\n\\nd(x, x) .\\n\\nFurthermore, add (x\" y,) to S;-l to produce Sa, i.e., S, = S,_l U {(x\"\\n\\n(1)\\n\\ny,n.\\n\\n\\x0c769\\n\\nAnother version improved to economize the memory is as follows.\\n\\nAlgorithm 2, At the initial stage, let So be composed of an arbitrary element\\nin X x Y. For every i = 1,2\"\", let ii-lex) for any x E X equal some y. such\\nthat (x?, y.) E Si-l and\\nd(x, x?) =\\n\\nmin\\n\\nd(x, x) .\\n\\n(i,i)ES.-l\\n\\nFurthermore, if ii-l(Xi) # Yi then let Si = Si-l, or add (Xi, Yi) to Si-l to\\nproduce Si, i.e., Si = Si-l U {(Xi, Yi)}\\'\\nIn either construction, ii approaches to f as i increases. However, the computation time\\ngrows proportionally to the size of Si. The second subject in constructing associative\\ndatabases is what addressing rule we should employ to economize the computation time. In\\nthe subsequent chapters, a construction of associative database for this purpose is proposed.\\nIt manages data in a form of binary tree.\\n\\nSELF-ORGANIZATION OF ASSOCIATIVE DATABASE\\nGiven a sample sequence (Xl, Yl), (X2\\' Y2), .. \" the algorithm for constructing associative\\ndatabase is as follows.\\n\\nAlgorithm 3,\\'\\n\\nStep I(Initialization): Let (x[root], y[root]) = (Xl, Yd. Here, x[.] and y[.] are\\nvariables assigned for respective nodes to memorize data.. Furthermore, let t = 1.\\nStep 2: Increase t by 1, and put x, in. After reset a pointer n to the root, repeat\\nthe following until n arrives at some terminal node, i.e., leaf.\\nNotations nand\\nd(xt, x[n)), let n\\n\\nn mean the descendant nodes of n.\\n=n. Otherwise, let n =n.\\n\\nIf d(x\" r[n)) ~\\n\\nStep 3: Display yIn] as the related information. Next, put y, in. If yIn] = y\" back\\nto step 2. Otherwise, first establish new descendant nodes n and n. Secondly,\\nlet\\n\\n(x[n], yIn))\\n(x[n], yIn))\\n\\n(x[n], yIn)),\\n(Xt, y,).\\n\\n(2)\\n(3)\\n\\nFinally, back to step 2. Here, the loop of step 2-3 can be stopped at any time\\nand also can be continued.\\nNow, suppose that gate elements, namely, artificial \"synapses\" that play the role of branching by d are prepared. Then, we obtain a new style of neural network with gate elements\\nbeing randomly connected by this algorithm.\\n\\nLETTER RECOGNITION\\nRecen tly, the vertical slitting method for recognizing typographic English letters3 , the\\nelastic matching method for recognizing hand written discrete English letters4 , the global\\ntraining and fuzzy logic search method for recognizing Chinese characters written in square\\nstyleS, etc. are published. The self-organization of associative database realizes the recognition of handwritten continuous English letters.\\n\\n\\x0c770\\n\\n9 /wn\"\\n\\nNOV\\n\\n~ ~ ~ -xk :La.t\\n\\n~~ ~ ~~~\\n\\ndw1lo\\'\\n\\n~~~~~of~~\\n\\n~~~ 4,-?~~4Fig. 1. Source document.\\n2~~---------------\\'\\n\\nlOO~---------------\\'\\n\\nH\\n\\no\\n\\no\\nFig. 2. Windowing.\\n\\n1000\\n\\n2000\\n\\n3000\\n\\n4000\\n\\nNumber of samples\\n\\no\\n\\n1000\\n\\n2000\\n\\n3000\\n\\n4000\\n\\nNUAlber of sampl es\\n\\nFig. 3. An experiment result.\\n\\nAn image scanner takes a document image (Fig. 1). The letter recognizer uses a parallelogram window that at least can cover the maximal letter (Fig. 2), and processes the\\nsequence of letters while shifting the window. That is, the recognizer scans a word in a\\nslant direction. And, it places the window so that its left vicinity may be on the first black\\npoint detected. Then, the window catches a letter and some part of the succeeding letter.\\nIf recognition of the head letter is performed, its end position, namely, the boundary line\\nbetween two letters becomes known. Hence, by starting the scanning from this boundary\\nand repeating the above operations, the recognizer accomplishes recursively the task. Thus\\nthe major problem comes to identifying the head letter in the window.\\nConsidering it, we define the following.\\n? Regard window images as x\\'s, and define X accordingly.\\n? For a (x, x) E X x X, denote by B a black point in the left area from the boundary on\\nwindow image X. Project each B onto window image x. Then, measure the Euclidean\\ndistance 6 between fj and a black point B on x being the closest to B. Let d(x, x) be\\nthe summation of 6\\'s for all black points B\\'s on x divided by the number of B\\'s.\\n? Regard couples of the \"reading\" and the position of boundary as y\\'s, and define Y\\naccordingly.\\nAn operator teaches the recognizer in interaction the relation between window image and\\nreading& boundary with algorithm 3. Precisely, if the recalled reading is incorrect, the\\noperator teaches a correct reading via the console. Moreover, if the boundary position is\\nincorrect, he teaches a correct position via the mouse.\\nFig. 1 shows partially a document image used in this experiment. Fig. 3 shows the\\nchange of the number of nodes and that of the recognition rate defined as the relative\\nfrequency of correct answers in the past 1000 trials. Speciiications of the window are height\\n= 20dot, width = 10dot, and slant angular = 68deg. In this example, the levels of tree\\nwere distributed in 6-19 at time 4000 and the recognition rate converged to about 74%.\\nExperimentally, the recognition rate converges to about 60-85% in most cases, and to 95% at\\na rare case. However, it does not attain 100% since, e.g., \"c\" and \"e\" are not distinguishable\\nbecause of excessive lluctuation in writing. If the consistency of the x, y-relation is not\\nassured like this, the number of nodes increases endlessly (d. Fig. 3). Hence, it is clever to\\nstop the learning when the recognition rate attains some upper limit. To improve further\\nthe recognition rate, we must consider the spelling of words. It is one of future subjects.\\n\\n\\x0c771\\n\\nOBSTACLE AVOIDING MOVEMENT\\nVarious systems of camera type autonomous mobile robot are reported flourishingly6-1O.\\nThe system made up by the authors (Fig. 4) also belongs to this category. Now, in mathematical methodologies, we solve usually the problem of obstacle avoiding movement as\\na cost minimization problem under some cost criterion established artificially. Contrarily,\\nthe self-organization of associative database reproduces faithfully the cost criterion of an\\noperator. Therefore, motion of the robot after learning becomes very natural.\\nNow, the length, width and height of the robot are all about O.7m, and the weight is\\nabout 30kg. The visual angle of camera is about 55deg. The robot has the following three\\nfactors of motion. It turns less than ?30deg, advances less than 1m, and controls speed less\\nthan 3km/h. The experiment was done on the passageway of wid th 2.5m inside a building\\nwhich the authors\\' laboratories exist in (Fig. 5). Because of an experimental intention, we\\narrange boxes, smoking stands, gas cylinders, stools, handcarts, etc. on the passage way at\\nrandom. We let the robot take an image through the camera, recall a similar image, and\\ntrace the route preliminarily recorded on it. For this purpose, we define the following.\\n? Let the camera face 28deg downward to take an image, and process it through a low\\npass filter. Scanning vertically the filtered image from the bottom to the top, search\\nthe first point C where the luminance changes excessively. Then, su bstitu te all points\\nfrom the bottom to C for white, and all points from C to the top for black (Fig. 6).\\n(If no obstacle exists just in front of the robot, the white area shows the \\'\\'free\\'\\' area\\nwhere the robot can move around.) Regard binary 32 x 32dot images processed thus\\nas x\\'s, and define X accordingly.\\n? For every (x, x) E X x X, let d(x, x) be the number of black points on the exclusive-or\\nimage between x and X.\\n? Regard as y\\'s the images obtained by drawing routes on images x\\'s, and define Y\\naccordingly.\\nThe robot superimposes, on the current camera image x, the route recalled for x, and\\ninquires the operator instructions. The operator judges subjectively whether the suggested\\nroute is appropriate or not. In the negative answer, he draws a desirable route on x with the\\nmouse to teach a new y to the robot. This opera.tion defines implicitly a sample sequence\\nof (x, y) reflecting the cost criterion of the operator.\\n\\n.::l\" !\\n-\\n\\nIibUBe\\n\\n_. -\\n\\n22\\n\\n11\\n\\nRoan\\n\\n12\\n\\n{-\\n\\n13\\n\\nStationary uni t\\n\\nFig. 4. Configuration of\\nautonomous mobile robot system.\\n\\n~\\n\\nI\\n\\n,\\n\\n23\\n\\n24\\n\\nNorth\\n14\\n\\nrmbi Ie unit (robot)\\n\\n-\\n\\nRoan\\n\\ny\\n\\nt\\n\\nFig. 5. Experimental\\nenvironment.\\n\\n\\x0c772\\n\\nWall\\n\\nCamera image\\n\\nPreprocessing\\n\\nA\\n\\n::: !fa\\n\\n?\\n\\nPreprocessing\\n\\n0\\n\\nO\\n\\nCourse\\nsuggest ion\\n\\n??\\n\\n..\\n\\nSearch\\n\\nA\\n\\nFig. 6. Processing for\\nobstacle avoiding movement.\\n\\nx\\n\\nFig. 1. Processing for\\nposition identification.\\nWe define the satisfaction rate by the relative frequency of acceptable suggestions of\\nroute in the past 100 trials. In a typical experiment, the change of satisfaction rate showed\\na similar tendency to Fig. 3, and it attains about 95% around time 800. Here, notice that\\nthe rest 5% does not mean directly the percentage of collision. (In practice, we prevent the\\ncollision by adopting some supplementary measure.) At time 800, the number of nodes was\\n145, and the levels of tree were distributed in 6-17.\\nThe proposed method reflects delicately various characters of operator. For example, a\\nrobot trained by an operator 0 moves slowly with enough space against obstacles while one\\ntrained by another operator 0\\' brushes quickly against obstacles. This fact gives us a hint\\non a method of printing \"characters\" into machines.\\nPOSITION IDENTIFICATION\\nThe robot can identify its position by recalling a similar landscape with the position data\\nto a camera image. For this purpose, in principle, it suffices to regard camera images and\\nposition data as x\\'s and y\\'s, respectively. However, the memory capacity is finite in actual\\ncompu ters. Hence, we cannot but compress the camera images at a slight loss of information.\\nSuch compression is admittable as long as the precision of position identification is in an\\nacceptable area. Thus, the major problem comes to find out some suitable compression\\nmethod.\\nIn the experimental environment (Fig. 5), juts are on the passageway at intervals of\\n3.6m, and each section between adjacent juts has at most one door. The robot identifies\\nroughly from a surrounding landscape which section itself places in. And, it uses temporarily\\na triangular surveying technique if an exact measure is necessary. To realize the former task,\\nwe define the following .\\n? Turn the camera to take a panorama image of 360deg. Scanning horizontally the\\ncenter line, substitute the points where the luminance excessively changes for black\\nand the other points for white (Fig. 1). Regard binary 360dot line images processed\\nthus as x\\'s, and define X accordingly.\\n? For every (x, x) E X x X, project each black point A on x onto x. And, measure the\\nEuclidean distance 6 between A and a black point A on x being the closest to A. Let\\nthe summation of 6 be S. Similarly, calculate S by exchanging the roles of x and X.\\nDenoting the numbers of A\\'s and A\\'s respectively by nand n, define\\n\\n\\x0c773\\n\\nd(x, x) =\\n\\n~(~\\n+ ~).\\n2 n\\nn\\n\\n(4)\\n\\n? Regard positive integers labeled on sections as y\\'s (cf. Fig. 5), and define Y accordingly.\\nIn the learning mode, the robot checks exactly its position with a counter that is reset periodically by the operator. The robot runs arbitrarily on the passageways within 18m area\\nand learns the relation between landscapes and position data. (Position identification beyond 18m area is achieved by crossing plural databases one another.) This task is automatic\\nexcepting the periodic reset of counter, namely, it is a kind of learning without teacher.\\nWe define the identification rate by the relative frequency of correct recalls of position\\ndata in the past 100 trials. In a typical example, it converged to about 83% around time\\n400. At time 400, the number of levels was 202, and the levels oftree were distributed in 522. Since the identification failures of 17% can be rejected by considering the trajectory, no\\npro blem arises in practical use. In order to improve the identification rate, the compression\\nratio of camera images must be loosened. Such possibility depends on improvement of the\\nhardware in the future.\\nFig. 8 shows an example of actual motion of the robot based on the database for obstacle\\navoiding movement and that for position identification. This example corresponds to a case\\nof moving from 14 to 23 in Fig. 5. Here, the time interval per frame is about 40sec.\\n\\n,~. .~ (\\n;~\"i..\\n~\\n\\n\"\\n\\n\"\\n\\n.\\n\\n..I\\n\\nI\\n\\n?\\n?\\n\\n\"\\n\\nI\\'\\n.\\n\\'.1\\nt\\n\\n;\\n\\ni\\n\\n-:\\n, . . , \\'II\\n\\nFig. 8. Actual motion of the robot.\\n\\n\\x0c774\\n\\nCONCLUSION\\nA method of self-organizing associative databases was proposed with the application to\\nrobot eyesight systems. The machine decomposes a global structure unknown into a set of\\nlocal structures known and learns universally any input-output response. This framework\\nof problem implies a wide application area other than the examples shown in this paper.\\nA defect of the algorithm 3 of self-organization is that the tree is balanced well only\\nfor a subclass of structures of f. A subject imposed us is to widen the class. A probable\\nsolution is to abolish the addressing rule depending directly on values of d and, instead, to\\nestablish another rule depending on the distribution function of values of d. It is now under\\ninvestigation.\\n\\nREFERENCES\\n1. Hopfield, J. J. and D. W. Tank, \"Computing with Neural Circuit: A Model/\\'\\n\\nScience 233 (1986), pp. 625-633.\\n2. Rumelhart, D. E. et al., \"Learning Representations by Back-Propagating Errors,\" Nature 323 (1986), pp. 533-536.\\n\\n3. Hull, J. J., \"Hypothesis Generation in a Computational Model for Visual Word\\nRecognition,\" IEEE Expert, Fall (1986), pp. 63-70.\\n4. Kurtzberg, J. M., \"Feature Analysis for Symbol Recognition by Elastic Matching,\" IBM J. Res. Develop. 31-1 (1987), pp. 91-95.\\n\\n5. Wang, Q. R. and C. Y. Suen, \"Large Tree Classifier with Heuristic Search and\\nGlobal Training,\" IEEE Trans. Pattern. Anal. & Mach. Intell. PAMI 9-1\\n(1987) pp. 91-102.\\n6. Brooks, R. A. et al, \"Self Calibration of Motion and Stereo Vision for Mobile\\nRobots,\" 4th Int. Symp. of Robotics Research (1987), pp. 267-276.\\n7. Goto, Y. and A. Stentz, \"The CMU System for Mobile Robot Navigation,\" 1987\\nIEEE Int. Conf. on Robotics & Automation (1987), pp. 99-105.\\n8. Madarasz, R. et al., \"The Design of an Autonomous Vehicle for the Disabled,\"\\nIEEE Jour. of Robotics & Automation RA 2-3 (1986), pp. 117-125.\\n9. Triendl, E. and D. J. Kriegman, \"Stereo Vision and Navigation within Buildings,\" 1987 IEEE Int. Conf. on Robotics & Automation (1987), pp. 1725-1730.\\n10. Turk, M. A. et al., \"Video Road-Following for the Autonomous Land Vehicle,\"\\n1987 IEEE Int. Conf. on Robotics & Automation (1987), pp. 273-279.\\n\\n\\x0c'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer"
      ],
      "metadata": {
        "id": "mWrVoWOQKTtX"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "MgTJb6qAKrjf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da93f9ea-5b99-4940-e294-53961cd01ccf"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = set(stopwords.words('english')) #set for unique stopwords, although the stopwords are unique"
      ],
      "metadata": {
        "id": "0wwWH_nqKVBg"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#creating custom stopwords\n",
        "new_words = [\"fig\", \"figure\", \"sample\", \"using\", \"shoe\", \"result\", \"Large\",\n",
        "             \"also\", \"one\", \"two\", \"three\", \"four\", \"five\", \"seven\", \"eight\", \"nine\"]"
      ],
      "metadata": {
        "id": "GXljrF97KXlc"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = list(stop_words.union(new_words))"
      ],
      "metadata": {
        "id": "0WNuBe2wLqyO"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "id": "XgBOb3b3NL0e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5f92e78-8ff0-4316-c4c2-ad3b676aad97"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocessing_text(txt):\n",
        "  txt = txt.lower()\n",
        "  txt = re.sub(r'<.*?>', ' ', txt)\n",
        "  txt = re.sub(r'[^a-zA-Z]', ' ', txt)\n",
        "  txt = nltk.word_tokenize(txt)\n",
        "  txt = [word for word in txt if word not in stop_words]\n",
        "  txt = [word for word in txt if len(word) > 3] #removing keywords conaining less than 3 alphabets\n",
        "  txt = [PorterStemmer().stem(word) for word in txt] #converting word into its base form (loving -> love)\n",
        "  txt = ' '.join(txt)\n",
        "\n",
        "  return txt"
      ],
      "metadata": {
        "id": "D1vzP3M3L_p9"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs = df['paper_text'].apply(preprocessing_text)"
      ],
      "metadata": {
        "id": "syYvguanRUKF"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs"
      ],
      "metadata": {
        "id": "fxmWvIpgCJ7U",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 458
        },
        "outputId": "c84fbf00-50f4-4409-b12e-75e75deaa0f9"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0       self organ associ databas applic hisashi suzuk...\n",
              "1       mean field theori layer visual cortex applic a...\n",
              "2       store covari associ long term potenti depress ...\n",
              "3       bayesian queri construct neural network model ...\n",
              "4       neural network ensembl cross valid activ learn...\n",
              "                              ...                        \n",
              "4995    rank time frequenc synthesi matthieu kowalski ...\n",
              "4996    state space model decod auditori attent modul ...\n",
              "4997    effici structur matrix rank minim adam wanli y...\n",
              "4998    cient minimax signal detect graph jing qian di...\n",
              "4999    signal aggreg constraint addit factori hmm app...\n",
              "Name: paper_text, Length: 5000, dtype: object"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>paper_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>self organ associ databas applic hisashi suzuk...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>mean field theori layer visual cortex applic a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>store covari associ long term potenti depress ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>bayesian queri construct neural network model ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>neural network ensembl cross valid activ learn...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4995</th>\n",
              "      <td>rank time frequenc synthesi matthieu kowalski ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4996</th>\n",
              "      <td>state space model decod auditori attent modul ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4997</th>\n",
              "      <td>effici structur matrix rank minim adam wanli y...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4998</th>\n",
              "      <td>cient minimax signal detect graph jing qian di...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4999</th>\n",
              "      <td>signal aggreg constraint addit factori hmm app...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5000 rows × 1 columns</p>\n",
              "</div><br><label><b>dtype:</b> object</label>"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "cv = CountVectorizer(max_df=95, max_features=5000, ngram_range=(1,3))\n",
        "word_count_vectors = cv.fit_transform(docs)"
      ],
      "metadata": {
        "id": "l4SAxYE490t0"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Document-Term Matrix Example\n",
        "\n",
        "**Documents:**\n",
        "\n",
        "1. \"The quick brown fox jumps over the lazy dog.\"\n",
        "2. \"The dog barks at the cat.\"\n",
        "3. \"The fox is quick and brown.\"\n",
        "\n",
        "**Vocabulary:**\n",
        "\n",
        "| Word | Index |\n",
        "|---|---|\n",
        "| quick | 0 |\n",
        "| brown | 1 |\n",
        "| fox | 2 |\n",
        "| jump | 3 |\n",
        "| lazi | 4 |\n",
        "| dog | 5 |\n",
        "| bark | 6 |\n",
        "| cat | 7 |\n",
        "\n",
        "\n",
        "**Document-Term Matrix:**\n",
        "\n",
        "| Document | quick | brown | fox | jump | lazi | dog | bark | cat |\n",
        "|---|---|---|---|---|---|---|---|---|\n",
        "| 1 | 1 | 1 | 1 | 1 | 1 | 1 | 0 | 0 |\n",
        "| 2 | 0 | 0 | 0 | 0 | 0 | 1 | 1 | 1 |\n",
        "| 3 | 1 | 1 | 1 | 0 | 0 | 0 | 0 | 0 |"
      ],
      "metadata": {
        "id": "gOfMW0X1qAUa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word_count_vectors"
      ],
      "metadata": {
        "id": "BM2cm_L3CHM0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f3cd7ac-2ab0-478f-cdc8-71600af96eda"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<5000x5000 sparse matrix of type '<class 'numpy.int64'>'\n",
              "\twith 332940 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Scenario: TF-IDF Calculation Example\n",
        "\n",
        "**Documents:**\n",
        "\n",
        "1. \"The cat sat on the mat.\"\n",
        "2. \"The dog chased the cat.\"\n",
        "3. \"The dog sat on the mat.\"\n",
        "\n",
        "**Steps:**\n",
        "\n",
        "**1. Count Vectorization:**\n",
        "\n",
        "We create a vocabulary and represent each document as a vector of word counts:\n",
        "\n",
        "| Document | cat | sat | on | mat | dog | chased |\n",
        "|---|---|---|---|---|---|---|\n",
        "| 1 | 1 | 1 | 1 | 1 | 0 | 0 |\n",
        "| 2 | 1 | 0 | 0 | 0 | 1 | 1 |\n",
        "| 3 | 0 | 1 | 1 | 1 | 1 | 0 |\n",
        "\n",
        "**2. TF-IDF Transformation:**\n",
        "\n",
        "We apply TF-IDF to the word counts:\n",
        "\n",
        "**TF (Term Frequency):**  Frequency of a word in a document.\n",
        "**IDF (Inverse Document Frequency):** Importance of a word across all documents.\n",
        "**TF-IDF:** TF * IDF\n",
        "\n",
        "**Example: Calculating TF-IDF for \"cat\" in Document 1:**\n",
        "\n",
        "* **TF:** 1 (count of \"cat\") / 5 (total words in Document 1) = 0.2\n",
        "* **IDF:** log(3 (total documents) / 2 (documents containing \"cat\")) ≈ 0.405\n",
        "* **TF-IDF:** 0.2 * 0.405 ≈ 0.081\n",
        "\n",
        "**Resulting TF-IDF Matrix:**\n",
        "\n",
        "| Document | cat | sat | on | mat | dog | chased |\n",
        "|---|---|---|---|---|---|---|\n",
        "| 1 | 0.081 | 0.06 | 0.06 | 0.06 | 0 | 0 |\n",
        "| 2 | 0.096 | 0 | 0 | 0 | 0.119 | 0.119 |\n",
        "| 3 | 0 | 0.086 | 0.086 | 0.086 | 0.139 | 0 |\n",
        "\n",
        "**Observation:**\n",
        "\n",
        "The word \"cat\" has higher TF-IDF values in documents where it appears (Documents 1 and 2) and lower values where it doesn't (Document 3). This highlights the importance of the word within specific documents and across the entire corpus."
      ],
      "metadata": {
        "id": "85nVAT9ortOq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "transformer = TfidfTransformer(smooth_idf=True, use_idf=True)\n",
        "tfidf_transformer = transformer.fit(word_count_vectors)"
      ],
      "metadata": {
        "id": "tFVldvRfCH1g"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_topn_from_vector(feature_names, sorted_items, topn=10):\n",
        "\n",
        "    sorted_items = sorted_items[:topn]\n",
        "\n",
        "    score_vals = []\n",
        "    feature_vals = []\n",
        "\n",
        "    for idx, score in sorted_items:\n",
        "        feature_vals.append(feature_names[idx])\n",
        "        score_vals.append(round(score, 3))\n",
        "\n",
        "    results= {}\n",
        "    for idx in range(len(feature_vals)):\n",
        "        results[feature_vals[idx]]=score_vals[idx]\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "9mM7l7MZwzbS"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Scenario:** Let's use a sample document:\n",
        "\n",
        "\"The quick brown fox jumps over the lazy dog. The dog barks.\"\n",
        "\n",
        "**1. `tfidf_vector.tocoo()`**\n",
        "\n",
        "After calculating TF-IDF scores, imagine `tfidf_vector` stores the following (simplified):\n",
        "\n",
        "| Word | Index | TF-IDF Score |\n",
        "|---|---|---|\n",
        "| the | 0 | 0.0 |\n",
        "| quick | 1 | 0.3 |\n",
        "| brown | 2 | 0.2 |\n",
        "| fox | 3 | 0.4 |\n",
        "| jumps | 4 | 0.1 |\n",
        "| over | 5 | 0.2 |\n",
        "| lazy | 6 | 0.6 |\n",
        "| dog | 7 | 0.5 |\n",
        "| barks | 8 | 0.0 |\n",
        "\n",
        "Applying `tocoo()` transforms this into COO format:\n",
        "\n",
        "| Row | Column (Word Index) | Value (TF-IDF Score) |\n",
        "|---|---|---|\n",
        "| 0 | 1 | 0.3 |\n",
        "| 0 | 2 | 0.2 |\n",
        "| 0 | 3 | 0.4 |\n",
        "| 0 | 4 | 0.1 |\n",
        "| 0 | 7 | 0.5 |\n",
        "| 0 | 5 | 0.2 |\n",
        "| 0 | 6 | 0.6 |\n",
        "| 0 | 8 | 0.0 |\n",
        "\n",
        "**2. `zip(tfidf_vector.tocoo().col, tfidf_vector.tocoo().data)`**\n",
        "\n",
        "This combines word indices and TF-IDF scores:\n",
        "\n",
        "> Output:\n",
        "[(1, 0.3), (2, 0.2), (3, 0.4), (4, 0.1), (7, 0.5), (5, 0.2), (6, 0.6), (8, 0.0)]\n",
        "\n",
        "\n",
        "Each tuple represents (word index, TF-IDF score).\n",
        "\n",
        "**3. `sorted(..., key=lambda x: x[1], reverse=True)`**\n",
        "\n",
        "This sorts the pairs based on TF-IDF scores (descending order):\n",
        "\n",
        "> Output:\n",
        "[(6, 0.6), (7, 0.5), (3, 0.4), (1, 0.3), (2, 0.2), (5, 0.2), (4, 0.1), (8, 0.0)]\n",
        "\n"
      ],
      "metadata": {
        "id": "iPnNcCG_ymWh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "feature_names = cv.get_feature_names_out()"
      ],
      "metadata": {
        "id": "8Av2Qo3w3Rcp"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_keywords(idx, docs):\n",
        "  tfidf_vector = tfidf_transformer.transform(cv.transform([docs[idx]]))\n",
        "\n",
        "  sorted_items = sorted(zip(tfidf_vector.tocoo().col, tfidf_vector.tocoo().data), key=lambda x: x[1], reverse=True)\n",
        "  keywords = extract_topn_from_vector(feature_names, sorted_items, 10)\n",
        "\n",
        "  return keywords"
      ],
      "metadata": {
        "id": "Xb02tHfqHPxa"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_keywords(idx, df, keywords):\n",
        "  print('\\n======title======')\n",
        "  print(df['title'][idx])\n",
        "  print('\\n======abstract======')\n",
        "  print(df['abstract'][idx])\n",
        "  print('\\n======keywords======')\n",
        "  for k in keywords:\n",
        "    print(k, keywords[k])"
      ],
      "metadata": {
        "id": "YDBRFin9DU7C"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "keywords = get_keywords(1, docs)\n",
        "print_keywords(1, df, keywords)"
      ],
      "metadata": {
        "id": "1-ED4KAkGMEq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "606ce175-996e-4d46-8c40-6ad091678d26"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======title======\n",
            "A Mean Field Theory of Layer IV of Visual Cortex and Its Application to Artificial Neural Networks\n",
            "\n",
            "======abstract======\n",
            "Abstract Missing\n",
            "\n",
            "======keywords======\n",
            "cortic cell 0.633\n",
            "monocular 0.334\n",
            "ocular domin 0.225\n",
            "ocular 0.201\n",
            "cell activ 0.196\n",
            "binocular 0.185\n",
            "singl cell 0.183\n",
            "hopfield model 0.146\n",
            "munro 0.143\n",
            "synapt modif 0.138\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "pickle.dump(cv, open('cv.pkl', 'wb'))\n",
        "pickle.dump(tfidf_transformer, open('tfidf.pkl', 'wb'))\n",
        "pickle.dump(feature_names, open('feature_names.pkl', 'wb'))"
      ],
      "metadata": {
        "id": "avMGbJiY2zzK"
      },
      "execution_count": 101,
      "outputs": []
    }
  ]
}